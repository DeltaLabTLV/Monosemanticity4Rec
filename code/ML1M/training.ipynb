{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqk6hGeAt4NC"
      },
      "source": [
        "# This notebook the training process is defined and executed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-jU-KhY6LHq"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "export_dir = os.getcwd()\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import plot\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "from scipy.special import expit  # Sigmoid function\n",
        "import itertools\n",
        "from IPython.display import Latex, display\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Ignore FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "# pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "test_flag = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzGfesjaP5zt"
      },
      "outputs": [],
      "source": [
        "pip install ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3XTmYGfP97r"
      },
      "outputs": [],
      "source": [
        "from ipynb.fs.defs.utils import *\n",
        "from ipynb.fs.defs.data_processing import *\n",
        "from ipynb.fs.defs.models import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6M-fL58mm_Z"
      },
      "source": [
        "## SAE Architecture for MF Latent Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqzHzVeLrN-6"
      },
      "source": [
        "Load the MF embedddings for users and items- input to SAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY1192JfmoXt"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from the .csv file\n",
        "df_item_emb_mf = pd.read_csv(Path(export_dir,'res_csv/ML1M/mf/items_embeddings_mf_model.csv'))\n",
        "df_user_emb_mf = pd.read_csv(Path(export_dir,'res_csv/ML1M/mf/users_embeddings_mf_model.csv'))\n",
        "df_b_i_emb_mf = pd.read_csv(Path(export_dir,'res_csv/ML1M/mf/b_i_mf_model.csv'))\n",
        "df_b_u_emb_mf = pd.read_csv(Path(export_dir,'res_csv/ML1M/mf/b_u_mf_model.csv'))\n",
        "\n",
        "\n",
        "# CONVERT TO TENSORS\n",
        "dataset_items_mf = torch.tensor(df_item_emb_mf.values, dtype=torch.float32)\n",
        "dataset_users_mf = torch.tensor(df_user_emb_mf.values, dtype=torch.float32)\n",
        "dataset_bu_mf = torch.tensor(df_b_u_emb_mf.values, dtype=torch.float32)\n",
        "dataset_bi_mf = torch.tensor(df_b_i_emb_mf.values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "interaction_embeddings = dataset_users_mf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmk5VbDrhqf7"
      },
      "source": [
        "## Test set sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0JgB0CJWNo-"
      },
      "outputs": [],
      "source": [
        "#uniform sampling:\n",
        "test_subset_users = random.sample(list(df_user_emb_mf.index), k=math.floor(df_user_emb_mf.shape[0]*0.2))\n",
        "test_subset_items = random.sample(list(df_item_emb_mf.index), k=math.floor(df_item_emb_mf.shape[0]*0.2))\n",
        "\n",
        "test_users_num = len(test_subset_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AOaZFLfWQdz"
      },
      "outputs": [],
      "source": [
        "train_subset_users = [i for i in df_user_emb_mf.index if i not in test_subset_users]\n",
        "interaction_embeddings = dataset_users[train_subset_users]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJVOyXbXmzjD"
      },
      "outputs": [],
      "source": [
        "def autoencoder_loss(\n",
        "    user: torch.Tensor,\n",
        "    user_data: torch.Tensor,\n",
        "    item: torch.Tensor,\n",
        "    item_data: torch.Tensor,\n",
        "    user_recons: torch.Tensor,\n",
        "    item_recons: torch.Tensor,\n",
        "    latent_activations_item: torch.Tensor,\n",
        "    latent_activations_user: torch.Tensor,\n",
        "    epochs_progress: float,\n",
        "    l1_weight: float=0.01,\n",
        "    kl_weight: float=0,\n",
        "    mse_weight = 0.1,\n",
        "    sparsity_target=0.05,\n",
        "    prediction_level_weight: float=0.0\n",
        "    )  -> int:\n",
        "\n",
        "\n",
        "    # prediction level loss term\n",
        "    b_i_mat =(dataset_bi_mf[item,:].T).repeat(len(user), 1)\n",
        "    b_u_mat =(dataset_bu_mf[user,:]).repeat( 1, len(item))\n",
        "    inner_orig = b_i_mat + b_u_mat + torch.from_numpy(user_data.dot(item_data.T))\n",
        "    inner_recons = b_i_mat + b_u_mat + torch.matmul(user_recons,item_recons.T)\n",
        "    prediction_level_loss = F.mse_loss(inner_recons, inner_orig)\n",
        "\n",
        "    # L2 reconstruction_loss term\n",
        "    l2_reconstruction_loss = F.mse_loss(user_recons, torch.from_numpy(user_data)) + F.mse_loss(item_recons, torch.from_numpy(item_data))\n",
        "\n",
        "    #---------------------------------\n",
        "\n",
        "    ## L1 sparsity loss term\n",
        "    l1_sparsity_loss_item = F.l1_loss(latent_activations_item, torch.zeros_like(latent_activations_item))\n",
        "    l1_sparsity_loss_user = F.l1_loss(latent_activations_user, torch.zeros_like(latent_activations_user))\n",
        "    l1_sparsity_loss = l1_sparsity_loss_item + l1_sparsity_loss_user\n",
        "\n",
        "\n",
        "    # Compute KL divergence sparsity loss term\n",
        "    kl_loss_user = kl_divergence_loss(latent_activations_user, sparsity_target)\n",
        "    kl_loss_item = kl_divergence_loss(latent_activations_item, sparsity_target)\n",
        "    kl_loss = kl_loss_user + kl_loss_item\n",
        "\n",
        "\n",
        "    # # Combine losses\n",
        "    if kl_weight ==0:\n",
        "      total_loss =  prediction_level_weight*prediction_level_loss +  mse_weight* l2_reconstruction_loss + l1_weight* l1_sparsity_loss\n",
        "    else: total_loss =  prediction_level_weight*prediction_level_loss+  mse_weight* l2_reconstruction_loss + kl_weight * kl_loss + l1_weight* l1_sparsity_loss\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpA3LcN2X3XO"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(dataset_users, dataset_items, test_data, input_dim, latent_dim,datset_hist=dataset_MF_SAE, num_epochs=18, batch_size=256, learning_rate=1e-3):\n",
        "    # autoencoder = Autoencoder(latent_dim, input_dim, activation=TopK(k=8), tied=True, normalize = True)\n",
        "    autoencoder = Autoencoder(latent_dim, input_dim, activation=nn.ReLU(), tied=True, normalize = True)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    # --- Create DataLoader for the user data\n",
        "    user_indices = torch.arange(dataset_users.shape[0]).unsqueeze(1)\n",
        "    dataset_users_wind = torch.cat((user_indices, dataset_users), dim=1)\n",
        "    dataloader_users = DataLoader(dataset_users_wind, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    # --- Create DataLoader for the item data\n",
        "    item_indices = torch.tensor(df_item_emb.index.tolist()).unsqueeze(1)\n",
        "    dataset_items_wind = torch.cat((item_indices, dataset_items), dim=1)\n",
        "    dataloader_items = DataLoader(dataset_items_wind, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        batch = 0\n",
        "        for data_users, data_items in zip(dataloader_users, itertools.cycle(dataloader_items)):\n",
        "\n",
        "\n",
        "            # data_users: shape [batch_size, 1 + input_dim]\n",
        "            user_idx  = data_users[:, 0].long()           # user indices, tensor\n",
        "            input_tensor_users = data_users[:, 1:]        # the actual features, tensor\n",
        "            normalized_users = normalize_matrix(input_tensor_users) # numpy.ndarray\n",
        "            input_tensor_users = pad_or_truncate_tensor(normalized_users, input_dim) # numpy.ndarray\n",
        "\n",
        "            latents_pre_act_usrs, latents_usrs, user_recons = autoencoder(input_tensor_users)\n",
        "\n",
        "\n",
        "            # data_items: shape [batch_size, 1 + input_dim]\n",
        "            item_idx = data_items[:, 0].long()\n",
        "            input_tensor_items = data_items[:, 1:]\n",
        "            normalized_items = normalize_matrix(input_tensor_items)\n",
        "            input_tensor_items = pad_or_truncate_tensor(normalized_items, input_dim)\n",
        "\n",
        "            latents_pre_act_items, latents_items, item_recons = autoencoder(input_tensor_items)\n",
        "\n",
        "            ephochs_progress = epoch/num_epochs\n",
        "            loss = autoencoder_loss(user_idx, input_tensor_users,item_idx, input_tensor_items,\n",
        "                    user_recons, item_recons,latents_items,latents_usrs,ephochs_progress,mse_weight=0.1,\n",
        "                                    l1_weight=0,kl_weight =0.01,prediction_level_weight = 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch+=1\n",
        "\n",
        "\n",
        "\n",
        "        (autoencoder.loss).append(loss.item())\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuNrii9ihCDV"
      },
      "source": [
        "with our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYf049Uyc8u6"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(dataset_users, dataset_items, test_data, test_subset_users, datset_hist= dataset_MF_SAE, input_dim, num_epochs=18 batch_size=256, learning_rate=1e-3):\n",
        "    # autoencoder = Autoencoder(latent_dim, input_dim, activation=TopK(k=8), tied=True, normalize = True)\n",
        "    autoencoder = Autoencoder(latent_dim, input_dim, activation=nn.ReLU(), tied=True, normalize = True)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # batch = 0\n",
        "        # len(datset_hist) - number of epochs\n",
        "        # len(datset_hist[0]) - number of batches at each epoch\n",
        "        for batch in range(len(datset_hist[epoch])):\n",
        "\n",
        "        # for data_users, data_items in zip(dataloader_users, itertools.cycle(dataloader_items)):\n",
        "            if test_flag == 1:\n",
        "              user_idx = [t[0] for t in datset_hist[epoch][batch] if t[0] not in test_subset_users]\n",
        "            else:\n",
        "              user_idx = [t[0] for t in datset_hist[epoch][batch]]\n",
        "\n",
        "            # if epoch == 0 and  len(user_idx)== 256:\n",
        "            #   users_in_batch.iloc[:,batch] = user_idx\n",
        "\n",
        "            # Get the second element of each tuple\n",
        "            item_idx = [t[1] for t in datset_hist[epoch][batch]]\n",
        "\n",
        "            # data_users: shape [batch_size, 1 + input_dim]\n",
        "            # user_idx  = data_users[:, 0].long()           # user indices, tensor\n",
        "            input_tensor_users = dataset_users[user_idx,:]\n",
        "            normalized_users = normalize_matrix(input_tensor_users) # numpy.ndarray\n",
        "            input_tensor_users = pad_or_truncate_tensor(normalized_users, input_dim) # numpy.ndarray\n",
        "            # print(input_tensor_users.shape)\n",
        "            latents_pre_act_usrs, latents_usrs, user_recons = autoencoder(input_tensor_users)\n",
        "\n",
        "            # data_items: shape [batch_size, 1 + input_dim]\n",
        "            # item_idx = data_items[:, 0].long()\n",
        "            input_tensor_items = dataset_items[item_idx, :]\n",
        "            normalized_items = normalize_matrix(input_tensor_items)\n",
        "            input_tensor_items = pad_or_truncate_tensor(normalized_items, input_dim)\n",
        "\n",
        "            latents_pre_act_items, latents_items, item_recons = autoencoder(input_tensor_items)\n",
        "\n",
        "            ephochs_progress = epoch/num_epochs\n",
        "            loss = autoencoder_loss(user_idx, input_tensor_users,item_idx, input_tensor_items,\n",
        "                    user_recons, item_recons,latents_items,latents_usrs,ephochs_progress,mse_weight=0.1,\n",
        "                                    l1_weight=0,kl_weight =0.01,inner_product_weight = 2)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        (autoencoder.loss).append(loss.item())\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgKcrVdqhC5h"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cdSn3x4Y1N_"
      },
      "outputs": [],
      "source": [
        "test_flag=1\n",
        "autoencoder=train_autoencoder(interaction_embeddings, dataset_items, dataset_users[test_subset_users], dataset_MF_SAE, input_dim=dataset_users.shape[1], latent_dim=22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsBCuCzThII5"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItN_uMtFX58u"
      },
      "outputs": [],
      "source": [
        "model_name = 'your_SAE_model_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB3z3PY6ZDWW"
      },
      "outputs": [],
      "source": [
        "# torch.save(autoencoder, Path(export_dir,f'models/ML1M/{model_name}.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhNrZ_HkHVlQ"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Q5XYZMnNCH"
      },
      "source": [
        "## SAE Architecture for NCF Latent Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IYA6zYJoF_h"
      },
      "source": [
        "load NCF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKVKKnFGnkDE"
      },
      "outputs": [],
      "source": [
        "# Initialize the model.\n",
        "model = NeuralCollaborativeFiltering(num_users=6039, num_items=3706,\n",
        "                                      embedding_dim=20, hidden_layers=[64, 32, 16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV_AGc4ni7v_"
      },
      "outputs": [],
      "source": [
        "# model = torch.load(Path(export_dir,'models/ML1M/NCF_recommender.pth'), weights_only=False)\n",
        "\n",
        "# # pos_idx_ex_use = model.pos_idx_ex_use\n",
        "# # pos_idx_ex_hidden = {(row): [item for item in pos_ex[row] if item not in pos_idx_ex_use[row]] for row in ratings_matrix.index}\n",
        "\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koTf72ZroK1A"
      },
      "source": [
        "SAE training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw4eEubZnNCI"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def train_sparse_autoencoder(model, sae_model,dataload , user_embed, item_embed,dataset_hist=dataset_NCF_SAE, epochs=18, lr=0.001, device='cpu',kl_weight, l2_weight, l1_weight=0, prediction_level_weight = 1):\n",
        "    model.to(device)\n",
        "    sae_model.to(device)\n",
        "    optimizer = optim.Adam(sae_model.parameters(), lr=lr)\n",
        "    mse_loss = nn.MSELoss()\n",
        "    ce_loss = nn.BCELoss()\n",
        "\n",
        "\n",
        "    # Set the fixed model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        batch = 0\n",
        "\n",
        "        samples_num = 0\n",
        "        for batch in range(len(dataset_hist[epoch])):\n",
        "\n",
        "            if test_flag == 1:\n",
        "              user = [t[0] for t in dataset_hist[epoch][batch] if t[0] not in test_subset_users and t[2]==1]\n",
        "            else:\n",
        "              user = [t[0] for t in dataset_hist[epoch][batch]]\n",
        "\n",
        "\n",
        "            #Get the second element of each tuple\n",
        "            if test_flag == 1:\n",
        "              item = [t[1] for t in dataset_hist[epoch][batch] if t[0] not in test_subset_users and t[2]==1]\n",
        "            else:\n",
        "              item = [t[1] for t in dataset_hist[epoch][batch]]\n",
        "\n",
        "\n",
        "            if test_flag == 1:\n",
        "              real_ratings = [t[2] for t in dataset_hist[epoch][batch] if t[0] not in test_subset_users and t[2]==1]\n",
        "            else:\n",
        "              real_ratings = [t[2] for t in dataset_hist[epoch][batch]]\n",
        "\n",
        "\n",
        "            user = torch.tensor(user, dtype=torch.long)\n",
        "            item = torch.tensor(item, dtype=torch.long)\n",
        "            real_ratings = torch.tensor(real_ratings, dtype=torch.float)\n",
        "\n",
        "            user = user.to(device)\n",
        "            item = item.to(device)\n",
        "            real_ratings = real_ratings.to(device)\n",
        "\n",
        "            # ---------------------------\n",
        "            # (a) Forward pass through fixed model with original embeddings:\n",
        "            user_emb = user_embed[user]   \n",
        "            item_emb = item_embed[item]  \n",
        "            x_full = torch.cat([user_emb, item_emb], dim=-1)  \n",
        "            y = model.fc_layers(x_full)               # (batch, 1)\n",
        "            y = y.squeeze()                           # (batch,)\n",
        "            # ---------------------------\n",
        "            # (b) Forward pass through SAE for each embedding individually:\n",
        "            user_rec, user_encoded = sae_model(user_emb)  \n",
        "            item_rec, item_encoded = sae_model(item_emb)  \n",
        "\n",
        "\n",
        "            # Compute reconstruction loss for each embedding.\n",
        "            l2_reconstruction_loss_user = mse_loss(user_rec, user_emb)\n",
        "            l2_reconstruction_loss_item = mse_loss(item_rec, item_emb)\n",
        "            l2_reconstruction_loss = l2_reconstruction_loss_user + l2_reconstruction_loss_item\n",
        "            # Sparsity penalty (L1 norm of the encoded activations)\n",
        "            l1_sparsity_loss_user = torch.mean(torch.abs(user_encoded))\n",
        "            l1_sparsity_loss_item = torch.mean(torch.abs(item_encoded))\n",
        "            l1_sparsity_loss = l1_sparsity_loss_user + l1_sparsity_loss_item\n",
        "\n",
        "\n",
        "            kl_loss_user = kl_divergence_loss(user_encoded)\n",
        "            kl_loss_item = kl_divergence_loss(item_encoded)\n",
        "            kl_loss =  (kl_loss_user + kl_loss_item)\n",
        "\n",
        "            # ---------------------------\n",
        "            # (c) Pass reconstructed embeddings through fixed model:\n",
        "            x_hat = torch.cat([user_rec, item_rec], dim=-1)  \n",
        "            y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "            y_hat = y_hat.squeeze()         # (batch,)\n",
        "\n",
        "\n",
        "\n",
        "            # prediction_level_loss difference between original output and reconstructed output.\n",
        "            prediction_level_loss = ce_loss(y_hat, y)\n",
        "            #------------------------------\n",
        "            recons_NCF_loss = ce_loss(y_hat, real_ratings)\n",
        "            #------------------------------\n",
        "\n",
        "            # Total loss:\n",
        "            loss = l2_weight*l2_reconstruction_loss + prediction_level_weight * prediction_level_loss +  l1_weight * l1_sparsity_loss + kl_weight * kl_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # Gradients flow from the fixed network (frozen) through SAE.\n",
        "            optimizer.step()\n",
        "            samples_num += len(real_ratings)\n",
        "\n",
        "            total_loss += loss.item() *  len(real_ratings) \n",
        "\n",
        "        avg_loss = total_loss / samples_num\n",
        "        (sae_model.loss).append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# =============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpllRnzloRwP"
      },
      "outputs": [],
      "source": [
        "test_flag=1\n",
        "sae_model = SparseAutoencoderNCF(input_dim=20, hidden_dim=22, topk=7, tie_weights=True)\n",
        "train_sparse_autoencoder(model, sae_model, batch_dataset, user_embeddings, item_embeddings,dataset_NCF_SAE, epochs=18, lr=0.001, device='cpu',kl_weight=0.07,l2_weight=2, l1_weight=0, prediction_level_weight=10)\n",
        "sae_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P32GUZmarGGY"
      },
      "outputs": [],
      "source": [
        "model_name = 'your_SAE_model_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ak8u1mJrH-d"
      },
      "outputs": [],
      "source": [
        "# torch.save(autoencoder, Path(export_dir,f'models/ML1M/{model_name}.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spCRSwsQHYjX"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyLso2Tm9PL"
      },
      "source": [
        "## MF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xlySSz9wZV0"
      },
      "outputs": [],
      "source": [
        "def train(mf_recommender, batch_size=256):\n",
        "    \"\"\"\n",
        "    Trains the model using mini-batch SGD.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(mf_recommender.iterations):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Re-sample negatives and build the one-hot matrix using class dimensions\n",
        "        neg_ex_unif = {\n",
        "            row: random.sample(mf_recommender.R.columns[mf_recommender.R.iloc[row] == 0].tolist(), mf_recommender.pos_ex_num[row])\n",
        "            for row in mf_recommender.R.index if mf_recommender.pos_ex_num[row] < mf_recommender.R.shape[1] / 2\n",
        "        }\n",
        "        neg_ex_popularity = {\n",
        "            row: list(np.random.choice(mf_recommender.neg_ex_use[row],\n",
        "              size=len(mf_recommender.pos_idx_ex_use[row]), replace=False, p=norm_prob_neg_use_exp[row]))\n",
        "            for row in mf_recommender.R.index\n",
        "        }\n",
        "        neg_idx_ex_use = neg_ex_popularity\n",
        "\n",
        "        # Build one-hot DataFrame with entries: 1 for positive, 0 for negatives, -1 otherwise\n",
        "        mf_recommender.df_1hot = pd.DataFrame(-1, index=mf_recommender.R.index, columns=mf_recommender.R.columns)\n",
        "        for row in range(mf_recommender.num_users):\n",
        "            mf_recommender.df_1hot.loc[row, mf_recommender.neg_idx_ex_use[row]] = 0\n",
        "            mf_recommender.df_1hot.loc[row, mf_recommender.pos_idx_ex_use[row]] = 1\n",
        "\n",
        "        # Create list of training samples: (user index, item index, rating, item id)\n",
        "        mf_recommender.samples = [\n",
        "            (i, j, mf_recommender.df_1hot.iloc[i, j], mf_recommender.df_1hot.columns[j])\n",
        "            for i in range(mf_recommender.num_users)\n",
        "            for j in range(mf_recommender.num_items)\n",
        "            if mf_recommender.df_1hot.iloc[i, j] != -1\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Shuffle the samples for mini-batch creation\n",
        "        random.shuffle(mf_recommender.samples)\n",
        "\n",
        "        # Process samples in mini-batches\n",
        "        for start in range(0, len(mf_recommender.samples), batch_size):\n",
        "            batch_samples = mf_recommender.samples[start:start+batch_size]\n",
        "            mf_recommender.sgd_batch(batch_samples)\n",
        "\n",
        "\n",
        "        rmse_val = mf_recommender.rmse()\n",
        "        print(f\"Epoch {epoch+1}; RMSE: {rmse_val:.4f}; epoch time: {time.time()-start_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDzYY1-HapC"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cslxnp6udDk"
      },
      "outputs": [],
      "source": [
        "mf_recommender = MatrixFactorization(ratings_matrix, pos_idx_ex_use,neg_idx_ex_use,neg_ex_hidden, neg_ex, pos_ex_num, K=20, alpha=0.05, beta=0.01, iterations=10, pop_flag = 1)\n",
        "\n",
        "start_time = time.time()\n",
        "mf_recommender.train(batch_size=256)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e_0v4yhuhlB"
      },
      "outputs": [],
      "source": [
        "model_name = 'your_recommender_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agHtftqgznEQ"
      },
      "outputs": [],
      "source": [
        "# with open(Path(export_dir,f'models/MF_model_{model_name}'), 'wb') as file:\n",
        "#     pickle.dump(mf_recommender, file)\n",
        "\n",
        "\n",
        "df_b_i= pd.DataFrame(mf_recommender.b_i)\n",
        "df_b_u= pd.DataFrame(mf_recommender.b_u)\n",
        "df_b_i.index = ratings_matrix.columns\n",
        "df_b_u.index = ratings_matrix.index\n",
        "\n",
        "\n",
        "df_b_i.to_csv(Path(export_dir,f'res_csv/b_i_{model_name}.csv'), index=False)\n",
        "# df_b_i.to_csv(Path(export_dir,f'res_csv/b_i_{model_name}_Windex.csv'))\n",
        "\n",
        "df_b_u.to_csv(Path(export_dir,f'res_csv/b_u_{model_name}.csv'), index=False)\n",
        "# df_b_u.to_csv(Path(export_dir,f'res_csv/b_u_{model_name}_Windex.csv'))\n",
        "\n",
        "pos_idx_ex_hidden_df= pd.DataFrame.from_dict(mf_recommender.pos_idx_ex_hidden, orient='index')\n",
        "pos_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "neg_idx_ex_hidden_df= pd.DataFrame.from_dict(mf_recommender.neg_ex_hidden, orient='index')\n",
        "neg_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/neg_test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "\n",
        "df_P= pd.DataFrame(mf_recommender.P)\n",
        "df_P.to_csv(Path(export_dir,f'res_csv/users_embeddings_{model_name}.csv'), index = False)\n",
        "\n",
        "\n",
        "df_Q= pd.DataFrame(mf_recommender.Q)\n",
        "df_Q.to_csv(Path(export_dir,f'res_csv/items_embeddings_{model_name}.csv'), index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxqfClGXHaxc"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4FJSbJ1Hazn"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjgkFFLBm-lg"
      },
      "source": [
        "## NCF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywa8PHXznMSU"
      },
      "outputs": [],
      "source": [
        "# Training loop for the model.\n",
        "def train_model(model, ratings, epochs=30, dataset_hist = dataset_NCF, lr=0.05, device='cpu'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    for epoch in range(epochs): # - number of epochs\n",
        "        epoch_loss = 0.0\n",
        "        start_time1 = time.time()\n",
        "\n",
        "\n",
        "        # in order to use our saved batch cstart commenr out here\n",
        "        neg_ex_unif = {(row): random.sample(ratings_matrix.columns\n",
        "            [ratings_matrix.iloc[row] == 0].tolist(),pos_ex_num[row]) for row in\n",
        "            ratings_matrix.index if model.pos_ex_num[row]<ratings_matrix.shape[1]/2}\n",
        "\n",
        "        # test set with neg and pos examples, distributed wrt popularity:\n",
        "        neg_ex_popularity = {(row): list(np.random.choice(neg_ex_use[row],\n",
        "                size=len(model.pos_idx_ex_use[row]),replace=False, p=norm_prob_neg_use_exp[row]))\n",
        "                    for row in ratings_matrix.index}\n",
        "        # change wrt unif/pop:\n",
        "        neg_idx_ex_use = neg_ex_popularity\n",
        "\n",
        "        df_1hot = pd.DataFrame(-1, index=ratings_matrix.index, columns=ratings_matrix.columns)\n",
        "        for row in range(ratings.shape[0]):\n",
        "          df_1hot.loc[row,model.neg_idx_ex_use[row]]=0\n",
        "          df_1hot.loc[row,model.pos_idx_ex_use[row]]=1\n",
        "\n",
        "        interactions = [(i, j, df_1hot.iloc[i,j], df_1hot.columns[j]) for\n",
        "            i in range(ratings.shape[0]) for j in range(ratings.shape[1]) if df_1hot.iloc[i,j]!=-1]\n",
        "\n",
        "        dataset = InteractionDataset(interactions)\n",
        "        dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "        counter=0\n",
        "        for user, item, rating, item_real_num in dataloader:\n",
        "            user, item, rating, item_real_num = user.to(device), item.to(device), rating.to(device), item_real_num.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(user, item)\n",
        "            loss = criterion(prediction, rating)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * user.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader.dataset) # num of batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, time: {time.time() - start_time1}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_evfcCBrf05"
      },
      "source": [
        "use our saved training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTVYNET0reIA"
      },
      "outputs": [],
      "source": [
        "# Training loop for the model.\n",
        "def train_model(model, ratings, epochs=30, dataset_hist = dataset_NCF, lr=0.05, device='cpu'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    for epoch in range(len(dataset_hist)): # - number of epochs\n",
        "        epoch_loss = 0.0\n",
        "        start_time1 = time.time()\n",
        "\n",
        "        #Load out Batch\n",
        "        for batch in range(len(dataset_hist[epoch])):\n",
        "            user = torch.tensor([t[0] for t in dataset_hist[epoch][batch]], dtype=torch.long)\n",
        "            item = torch.tensor([t[1] for t in dataset_hist[epoch][batch]], dtype=torch.long)\n",
        "            rating= torch.tensor([t[2] for t in dataset_hist[epoch][batch]], dtype=torch.float)\n",
        "            item_real_num = torch.tensor([t[3] for t in dataset_hist[epoch][batch]], dtype=torch.long)\n",
        "            user, item, rating, item_real_num = user.to(device), item.to(device), rating.to(device), item_real_num.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(user, item)\n",
        "            loss = criterion(prediction, rating)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * user.size(0)\n",
        "\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataset_hist[epoch])  # num of batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, time: {time.time() - start_time1}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJZ79231stWl"
      },
      "source": [
        "trainings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaXhAa3x7Lwt"
      },
      "outputs": [],
      "source": [
        "# Initialize the model.\n",
        "model = NeuralCollaborativeFiltering(num_users=6039, num_items=3706,\n",
        "                                      embedding_dim=20, hidden_layers=[64, 32, 16])\n",
        "# Train the model.\n",
        "train_model(model, ratings_matrix, epochs=30, dataset_NCF, lr=0.05, device='cpu')\n",
        "\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AG5BuV7-5ay"
      },
      "outputs": [],
      "source": [
        "model_name = 'your NCF recommender name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8oISZdllWRi"
      },
      "outputs": [],
      "source": [
        "# torch.save(model, Path(export_dir,'models/ML1M/model_name.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd1J8_Tn9otP"
      },
      "outputs": [],
      "source": [
        "user_embeddings = model.user_embedding.weight.detach().clone()  # Tensor of shape (num_users, embedding_dim)\n",
        "item_embeddings = model.item_embedding.weight.detach().clone()  # Tensor of shape (num_items, embedding_dim)\n",
        "\n",
        "\n",
        "pos_idx_ex_hidden_df= pd.DataFrame.from_dict(model.pos_idx_ex_hidden, orient='index')\n",
        "pos_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "neg_idx_ex_hidden_df= pd.DataFrame.from_dict(model.neg_ex_hidden, orient='index')\n",
        "neg_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/neg_test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "\n",
        "user_embeddings.to_csv(Path(export_dir,f'res_csv/NCF_{model_name}_user_embeddings.csv'), index = False)\n",
        "item_embeddings.to_csv(Path(export_dir,f'res_csv/NCF_{model_name}_item_embeddings.csv'), index = False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
