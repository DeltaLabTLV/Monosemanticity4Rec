{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piiC_D4HhOAc"
      },
      "source": [
        "## This notebook contains models architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-jU-KhY6LHq"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "export_dir = os.getcwd()\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import plot\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "from scipy.special import expit  # Sigmoid function\n",
        "import itertools\n",
        "from IPython.display import Latex, display\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Ignore FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "torch.set_printoptions(sci_mode=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rvEWZkEhWte"
      },
      "outputs": [],
      "source": [
        "pip install ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1CmF7q1h6iF"
      },
      "outputs": [],
      "source": [
        "from ipynb.fs.defs.utils import *\n",
        "from ipynb.fs.defs.data_processing import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6M-fL58mm_Z"
      },
      "source": [
        "## SAE Architecture for NCF Latent Concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY1192JfmoXt"
      },
      "outputs": [],
      "source": [
        "class TopKActivation(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        \"\"\"\n",
        "        Keeps only the top k values (per sample) and zeros out the rest.\n",
        "        \"\"\"\n",
        "        super(TopKActivation, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Get top-k values along the feature dimension.\n",
        "        topk_values, _ = torch.topk(x, self.k, dim=1)\n",
        "        # Threshold is the k-th largest value for each sample.\n",
        "        threshold = topk_values[:, -1].unsqueeze(1).expand_as(x)\n",
        "        mask = (x >= threshold).float()\n",
        "        return x * mask\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Custom Tied Transpose Module for Decoder\n",
        "class TiedTranspose(nn.Module):\n",
        "    def __init__(self, tied_layer):\n",
        "        \"\"\"\n",
        "        Ties this module's weight to the transpose of the given linear layer's weight.\n",
        "        \"\"\"\n",
        "        super(TiedTranspose, self).__init__()\n",
        "        self.tied_layer = tied_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use the transpose of the tied layer's weight.\n",
        "        return nn.functional.linear(x, self.tied_layer.weight.t())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJVOyXbXmzjD"
      },
      "outputs": [],
      "source": [
        "class SparseAutoencoderNCF(nn.Module):\n",
        "    def __init__(self, input_dim=20, hidden_dim=22, topk=5, tie_weights=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of each input embedding (20).\n",
        "            hidden_dim (int): Dimensionality of the latent space.\n",
        "            topk (int): Number of activations to keep per sample- if using topK activation.\n",
        "            tie_weights (bool): If True, tie the decoder's weight to the encoder's weight.\n",
        "        \"\"\"\n",
        "        super(SparseAutoencoderNCF, self).__init__()\n",
        "        self.encoder_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        self.topk_activation = nn.ReLU()\n",
        "\n",
        "        self.tie_weights = tie_weights\n",
        "        if tie_weights:\n",
        "            self.decoder = TiedTranspose(self.encoder_linear)\n",
        "        else:\n",
        "            self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
        "        self.loss = []\n",
        "        self.weights_loss = []\n",
        "        self.test_subset_users_ind = test_subset_users\n",
        "        self.test_subset_items_ind = test_subset_items\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        latent_pre_act = self.encoder_linear(x)           # (batch, hidden_dim)\n",
        "        encoded = self.topk_activation(latent_pre_act)      # (batch, hidden_dim)\n",
        "        decoded = self.decoder(encoded)                     # (batch, input_dim)\n",
        "        return decoded, encoded\n",
        "\n",
        "\n",
        "class SAEInteractionDataset(Dataset):\n",
        "    def __init__(self, interactions):\n",
        "        \"\"\"\n",
        "        interactions: list of tuples (user_id, item_id)\n",
        "        \"\"\"\n",
        "        self.interactions = interactions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user, item = self.interactions[idx]\n",
        "        return torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5EER_PmHV5K"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Q5XYZMnNCH"
      },
      "source": [
        "## SAE Architecture for MF Latent Concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw4eEubZnNCI"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "\n",
        "      def __init__(\n",
        "          self, n_latents: int, n_inputs: int, activation: Callable = nn.ReLU(), tied: bool = True,\n",
        "          normalize: bool = False\n",
        "      ) -> None:\n",
        "          super().__init__()\n",
        "\n",
        "          self.pre_bias = nn.Parameter(torch.zeros(n_inputs))\n",
        "          self.encoder: nn.Module = nn.Linear(n_inputs, n_latents, bias=False)\n",
        "          self.latent_bias = nn.Parameter(torch.zeros(n_latents))\n",
        "          self.activation = activation\n",
        "          if tied:\n",
        "              self.decoder: nn.Linear | TiedTranspose = TiedTranspose(self.encoder)\n",
        "          else:\n",
        "              self.decoder = nn.Linear(n_latents, n_inputs, bias=False)\n",
        "          self.normalize = normalize\n",
        "          self.loss = []\n",
        "          self.test_subset_users_ind = test_subset_users ####\n",
        "          self.test_subset_items_ind = test_subset_items ####\n",
        "          self.weights_loss = [] ####\n",
        "          self.test = test_flag\n",
        "\n",
        "\n",
        "      def encode_pre_act(self, x: torch.Tensor, latent_slice: slice = slice(None)) -> torch.Tensor:\n",
        "          if type(x) == np.ndarray:\n",
        "            x= torch.from_numpy(x)\n",
        "          x = x - self.pre_bias\n",
        "          latents_pre_act = F.linear(\n",
        "              x, self.encoder.weight[latent_slice], self.latent_bias[latent_slice]\n",
        "          )\n",
        "          return latents_pre_act\n",
        "\n",
        "      def preprocess(self, x: torch.Tensor) -> tuple[torch.Tensor, dict[str, Any]]:\n",
        "          if not self.normalize:\n",
        "              return x, dict()\n",
        "          x, mu, std = LN(x)\n",
        "          return x, dict(mu=mu, std=std)\n",
        "\n",
        "      def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, dict[str, Any]]:\n",
        "          x, info = self.preprocess(x)\n",
        "          return self.activation(self.encode_pre_act(x)), info\n",
        "\n",
        "      def decode(self, latents: torch.Tensor, info: dict[str, Any] | None = None) -> torch.Tensor:\n",
        "          ret = self.decoder(latents) + self.pre_bias\n",
        "          if self.normalize:\n",
        "              assert info is not None\n",
        "              ret = ret * info[\"std\"] + info[\"mu\"]\n",
        "          return ret\n",
        "\n",
        "      def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "          x, info = self.preprocess(x)\n",
        "          latents_pre_act = self.encode_pre_act(x)\n",
        "          latents = self.activation(latents_pre_act)\n",
        "          recons = self.decode(latents, info)\n",
        "          return latents_pre_act, latents, recons\n",
        "\n",
        "      @classmethod\n",
        "      def from_state_dict(\n",
        "          cls, state_dict: dict[str, torch.Tensor], strict: bool = True\n",
        "      ) -> \"Autoencoder\":\n",
        "          n_latents, d_model = state_dict[\"encoder.weight\"].shape\n",
        "\n",
        "          # Retrieve activation\n",
        "          activation_class_name = state_dict.pop(\"activation\", \"ReLU\")\n",
        "          activation_class = ACTIVATIONS_CLASSES.get(activation_class_name, nn.ReLU)\n",
        "          normalize = activation_class_name == \"TopK\"\n",
        "          activation_state_dict = state_dict.pop(\"activation_state_dict\", {})\n",
        "          if hasattr(activation_class, \"from_state_dict\"):\n",
        "              activation = activation_class.from_state_dict(\n",
        "                  activation_state_dict, strict=strict\n",
        "              )\n",
        "          else:\n",
        "              activation = activation_class()\n",
        "              if hasattr(activation, \"load_state_dict\"):\n",
        "                  activation.load_state_dict(activation_state_dict, strict=strict)\n",
        "\n",
        "          autoencoder = cls(n_latents, d_model, activation=activation, normalize=normalize)\n",
        "          autoencoder.load_state_dict(state_dict, strict=strict)\n",
        "          return autoencoder\n",
        "\n",
        "      def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n",
        "          sd = super().state_dict(destination, prefix, keep_vars)\n",
        "          sd[prefix + \"activation\"] = self.activation.__class__.__name__\n",
        "          if hasattr(self.activation, \"state_dict\"):\n",
        "              sd[prefix + \"activation_state_dict\"] = self.activation.state_dict()\n",
        "          return sd\n",
        "\n",
        "\n",
        "class TiedTranspose(nn.Module):\n",
        "    def __init__(self, linear: nn.Linear):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert self.linear.bias is None\n",
        "        return F.linear(x, self.linear.weight.t(), None)\n",
        "\n",
        "    @property\n",
        "    def weight(self) -> torch.Tensor:\n",
        "        return self.linear.weight.t()\n",
        "\n",
        "    @property\n",
        "    def bias(self) -> torch.Tensor:\n",
        "        return self.linear.bias\n",
        "\n",
        "\n",
        "class TopK(nn.Module):\n",
        "    def __init__(self, k: int, postact_fn: Callable = nn.ReLU()) -> None:\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.postact_fn = postact_fn\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        topk = torch.topk(x, k=self.k, dim=-1)\n",
        "        values = self.postact_fn(topk.values)\n",
        "        result = torch.zeros_like(x)\n",
        "        result.scatter_(-1, topk.indices, values)\n",
        "        return result\n",
        "\n",
        "    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n",
        "        state_dict = super().state_dict(destination, prefix, keep_vars)\n",
        "        state_dict.update({prefix + \"k\": self.k, prefix + \"postact_fn\": self.postact_fn.__class__.__name__})\n",
        "        return state_dict\n",
        "\n",
        "    @classmethod\n",
        "    def from_state_dict(cls, state_dict: dict[str, torch.Tensor], strict: bool = True) -> \"TopK\":\n",
        "        k = state_dict[\"k\"]\n",
        "        postact_fn = ACTIVATIONS_CLASSES[state_dict[\"postact_fn\"]]()\n",
        "        return cls(k=k, postact_fn=postact_fn)\n",
        "\n",
        "\n",
        "ACTIVATIONS_CLASSES = {\n",
        "    \"ReLU\": nn.ReLU,\n",
        "    \"Identity\": nn.Identity,\n",
        "    \"TopK\": TopK,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40FXnwFoZRXN"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyLso2Tm9PL"
      },
      "source": [
        "## MF recommender Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xlySSz9wZV0"
      },
      "outputs": [],
      "source": [
        "class MatrixFactorization:\n",
        "    def __init__(self, R, pos_idx_ex_use, neg_idx_ex_use, neg_ex_hidden, neg_ex, pos_ex_num, K, alpha, beta, iterations, pop_flag):\n",
        "        self.R = R\n",
        "        self.ratings = R\n",
        "        self.num_users = R.shape[0]\n",
        "        self.num_items = R.shape[1]\n",
        "        self.K = K  # latent dimensions (e.g., 20)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.iterations = iterations\n",
        "\n",
        "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
        "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
        "\n",
        "        # Initialize biases\n",
        "        self.b_u = np.zeros(self.num_users)\n",
        "        self.b_i = np.zeros(self.num_items)\n",
        "        self.b = np.mean(self.R[self.R > 0])\n",
        "\n",
        "        self.pos_idx_ex_use = pos_idx_ex_use\n",
        "        self.neg_idx_ex_use = neg_idx_ex_use\n",
        "        self.neg_ex = neg_ex\n",
        "        self.neg_ex_hidden = neg_ex_hidden\n",
        "        self.neg_ex_use = {(row): list(filter(lambda x: x not in self.neg_ex_hidden[row],\n",
        "                                self.neg_ex[row])) for row in self.R.index}\n",
        "        self.pos_ex_num = pos_ex_num\n",
        "        self.pop_flag = pop_flag\n",
        "\n",
        "        self.columns = list(self.ratings.columns)\n",
        "        self.batch_history = []\n",
        "\n",
        "    def sgd_batch(self, batch_samples):\n",
        "        # Convert mini-batch samples to arrays\n",
        "        users = np.array([s[0] for s in batch_samples], dtype=int)\n",
        "        items = np.array([s[1] for s in batch_samples], dtype=int)\n",
        "        ratings = np.array([s[2] for s in batch_samples], dtype=float)\n",
        "\n",
        "        # Compute predictions vectorized\n",
        "        predictions = expit(self.b + self.b_u[users] + self.b_i[items] + np.sum(self.P[users] * self.Q[items], axis=1))\n",
        "        errors = ratings - predictions  # error vector\n",
        "\n",
        "        # Compute gradients for biases and latent factors\n",
        "        grad_b_u = self.alpha * (errors - self.beta * self.b_u[users])\n",
        "        grad_b_i = self.alpha * (errors - self.beta * self.b_i[items])\n",
        "        grad_P = self.alpha * (errors[:, np.newaxis] * self.Q[items] - self.beta * self.P[users])\n",
        "        grad_Q = self.alpha * (errors[:, np.newaxis] * self.P[users] - self.beta * self.Q[items])\n",
        "\n",
        "        # Use np.add.at to accumulate updates correctly for duplicate indices\n",
        "        np.add.at(self.b_u, users, grad_b_u)\n",
        "        np.add.at(self.b_i, items, grad_b_i)\n",
        "        np.add.at(self.P, users, grad_P)\n",
        "        np.add.at(self.Q, items, grad_Q)\n",
        "\n",
        "    def rmse(self):\n",
        "        # Compute full prediction matrix\n",
        "        predicted = expit(self.b + self.b_u[:, np.newaxis] + self.b_i[np.newaxis, :] + self.P.dot(self.Q.T))\n",
        "        error = 0\n",
        "        count = 0\n",
        "        # Only consider positive examples (rating==1)\n",
        "        for i, k, r, _ in self.samples:\n",
        "            if r == 1:\n",
        "                error += (1 - predicted[i, k]) ** 2\n",
        "                count += 1\n",
        "        return np.sqrt(error / count) if count > 0 else None\n",
        "\n",
        "    def get_rating(self, i, j):\n",
        "        return expit(self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T))\n",
        "\n",
        "    def full_matrix(self):\n",
        "        return expit(self.b + self.b_u[:, np.newaxis] + self.b_i[np.newaxis, :] + self.P.dot(self.Q.T))\n",
        "\n",
        "    def recommend(self, user_id, top_n):\n",
        "        df_user_ratings = pd.DataFrame(self.full_matrix(), index=self.R.index, columns=self.R.columns)\n",
        "        df_user_ratings_one = df_user_ratings.loc[user_id]\n",
        "        recommendations = [(i, df_user_ratings_one.loc[i]) for i in df_user_ratings.columns]\n",
        "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "        return recommendations[:top_n]\n",
        "\n",
        "    def recommend_norm(self, user_id, top_n):\n",
        "        return normalize_val(self.recommend(user_id, top_n))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gApW9fkZP1S"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjgkFFLBm-lg"
      },
      "source": [
        "## NCF recommender Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywa8PHXznMSU"
      },
      "outputs": [],
      "source": [
        "# Dataset for user-item interactions.\n",
        "\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, interactions):\n",
        "        \"\"\"\n",
        "        interactions: list of tuples (user_id, item_id, rating)\n",
        "        \"\"\"\n",
        "        self.interactions = interactions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user, item, rating, item_real_num = self.interactions[index]\n",
        "        # Convert user and item to tensor and rating to float tensor\n",
        "        return torch.tensor(user, dtype=torch.long), \\\n",
        "               torch.tensor(item, dtype=torch.long), \\\n",
        "               torch.tensor(rating, dtype=torch.float), \\\n",
        "               torch.tensor(item_real_num, dtype=torch.long)\n",
        "\n",
        "class NeuralCollaborativeFiltering(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=20, hidden_layers=[64, 32, 16]):\n",
        "        \"\"\"\n",
        "        num_users: total number of users\n",
        "        num_items: total number of items\n",
        "        embedding_dim: dimensionality of the latent representation\n",
        "        hidden_layers: list containing the sizes of the hidden layers of the MLP\n",
        "        \"\"\"\n",
        "        super(NeuralCollaborativeFiltering, self).__init__()\n",
        "\n",
        "\n",
        "        #------------------------------------------------------------\n",
        "        # item and user embeddings initializtion:\n",
        "\n",
        "        # pretrained_weights_item = dataset_items_init.clone().detach()\n",
        "        # # Create the embedding layer:\n",
        "        # item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        # # Copy the pretrained weights into the embedding layer:\n",
        "        # item_embedding.weight.data.copy_(pretrained_weights_item)\n",
        "\n",
        "\n",
        "        # pretrained_weights_user = dataset_users_init.clone().detach()\n",
        "        # # Create the embedding layer:\n",
        "        # user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        # # Copy the pretrained weights into the embedding layer:\n",
        "        # user_embedding.weight.data.copy_(pretrained_weights_user)\n",
        "\n",
        "        self.user_embedding = user_embedding\n",
        "        self.item_embedding = item_embedding\n",
        "\n",
        "        nn.init.normal_(self.user_embedding.weight, mean=0.0, std=0.1)\n",
        "        nn.init.normal_(self.item_embedding.weight, mean=0.0, std=0.1)\n",
        "\n",
        "        #------------------------------------------------------------\n",
        "\n",
        "        self.pos_idx_ex_use = pos_idx_ex_use\n",
        "        self.neg_idx_ex_use = neg_idx_ex_use\n",
        "        self.neg_ex = neg_ex\n",
        "        self.neg_ex_hidden= neg_ex_hidden\n",
        "        self.pos_ex_num = pos_ex_num\n",
        "\n",
        "        # MLP:\n",
        "        layers = []\n",
        "        input_dim = embedding_dim * 2  # Concatenation of user and item embeddings\n",
        "        for hidden_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_dim = hidden_dim\n",
        "        # Final output layer to produce the rating or score\n",
        "        layers.append(nn.Linear(input_dim, 1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.fc_layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        # Look up embeddings\n",
        "        user_emb = self.user_embedding(user_indices)\n",
        "        item_emb = self.item_embedding(item_indices)\n",
        "        # Concatenate embeddings instead of taking inner product.\n",
        "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
        "        # Pass through the MLP to obtain prediction.\n",
        "        output = self.fc_layers(x)\n",
        "        return output.squeeze()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
