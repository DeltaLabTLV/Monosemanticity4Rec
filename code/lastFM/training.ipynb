{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqk6hGeAt4NC"
      },
      "source": [
        "# This notebook the training process is defined and executed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-jU-KhY6LHq"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "export_dir = os.getcwd()\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import plot\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "from scipy.special import expit  # Sigmoid function\n",
        "import itertools\n",
        "from IPython.display import Latex, display\n",
        "import pickle\n",
        "import warnings\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "\n",
        "\n",
        "# Ignore FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "# pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "test_flag = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzGfesjaP5zt"
      },
      "outputs": [],
      "source": [
        "pip install ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3XTmYGfP97r"
      },
      "outputs": [],
      "source": [
        "from ipynb.fs.defs.utils import *\n",
        "from ipynb.fs.defs.data_processing import *\n",
        "from ipynb.fs.defs.models import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6M-fL58mm_Z"
      },
      "source": [
        "## SAE MF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqzHzVeLrN-6"
      },
      "source": [
        "Load the MF embedddings for users and items- input to SAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY1192JfmoXt"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from the .csv file\n",
        "df_item_emb_mf = pd.read_csv(Path(export_dir,'res_csv/lastFM/mf/items_embeddings_mf_model.csv'))\n",
        "df_user_emb_mf = pd.read_csv(Path(export_dir,'res_csv/lastFM/mf/users_embeddings_mf_model.csv'))\n",
        "df_b_i_emb_mf = pd.read_csv(Path(export_dir,'res_csv/lastFM/mf/b_i_mf_model.csv'))\n",
        "df_b_u_emb_mf = pd.read_csv(Path(export_dir,'res_csv/lastFM/mf/b_u_mf_model.csv'))\n",
        "\n",
        "\n",
        "# CONVERT TO TENSORS\n",
        "dataset_items_mf = torch.tensor(df_item_emb_mf.values, dtype=torch.float32)\n",
        "dataset_users_mf = torch.tensor(df_user_emb_mf.values, dtype=torch.float32)\n",
        "dataset_bu_mf = torch.tensor(df_b_u_emb_mf.values, dtype=torch.float32)\n",
        "dataset_bi_mf = torch.tensor(df_b_i_emb_mf.values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "interaction_embeddings = dataset_users_mf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmk5VbDrhqf7"
      },
      "source": [
        "## Test set sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0JgB0CJWNo-"
      },
      "outputs": [],
      "source": [
        "#uniform sampling:\n",
        "test_subset_users = random.sample(list(df_user_emb_mf.index), k=math.floor(df_user_emb_mf.shape[0]*0.2))\n",
        "test_subset_items = random.sample(list(df_item_emb_mf.index), k=math.floor(df_item_emb_mf.shape[0]*0.2))\n",
        "\n",
        "test_users_num = len(test_subset_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AOaZFLfWQdz"
      },
      "outputs": [],
      "source": [
        "train_subset_users = [i for i in df_user_emb_mf.index if i not in test_subset_users]\n",
        "interaction_embeddings = dataset_users[train_subset_users]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJVOyXbXmzjD"
      },
      "outputs": [],
      "source": [
        "def autoencoder_loss(\n",
        "    user: torch.Tensor,\n",
        "    user_data:torch.Tensor,\n",
        "    item: torch.Tensor,\n",
        "    item_data: torch.Tensor,\n",
        "    user_recons:torch.Tensor,\n",
        "    item_recons: torch.Tensor,\n",
        "    latent_activations_user: torch.Tensor,\n",
        "    latent_activations_item: torch.Tensor,\n",
        "    epochs_progress: float,\n",
        "    l1_weight: float=1.2,\n",
        "    kl_weight: float=0.7,\n",
        "    mse_weight = 2.6,\n",
        "    inner_product_weight: float=1.0,\n",
        "    sparsity_target=0.1\n",
        "    )  -> int:\n",
        "\n",
        "    inner_orig = user_data@(item_data.T)\n",
        "    inner_recons = torch.matmul(user_recons,item_recons.T) \n",
        "\n",
        "    # # Compute L1 sparsity loss\n",
        "    sparsity_loss_item = F.l1_loss(latent_activations_item, torch.zeros_like(latent_activations_item))\n",
        "    sparsity_loss_user = F.l1_loss(latent_activations_user, torch.zeros_like(latent_activations_user))\n",
        "\n",
        "    # Compute KL divergence sparsity loss\n",
        "    kl_loss_user = kl_divergence_loss(latent_activations_user, sparsity_target)\n",
        "    kl_loss_item = kl_divergence_loss(latent_activations_item, sparsity_target)\n",
        "\n",
        "    reconstruction_loss = F.mse_loss(user_recons, user_data) + F.mse_loss(item_recons, item_data)\n",
        "\n",
        "    if kl_weight ==0:\n",
        "      total_loss =  mse_weight* reconstruction_loss+ l1_weight* (sparsity_loss_item+sparsity_loss_user)+inner_product_weight*F.mse_loss(inner_recons, inner_orig)\n",
        "    else: total_loss = mse_weight* reconstruction_loss + kl_weight * (kl_loss_item+kl_loss_user)+ l1_weight* (sparsity_loss_item+sparsity_loss_user)+ inner_product_weight*F.mse_loss(inner_recons, inner_orig)\n",
        "\n",
        "    return total_loss, reconstruction_loss,(mse_weight* reconstruction_loss).detach(), (l1_weight* (sparsity_loss_item+sparsity_loss_user)).detach(), (kl_weight * (kl_loss_item+kl_loss_user)).detach(), (inner_product_weight*F.mse_loss(inner_recons, inner_orig)).detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpA3LcN2X3XO"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(dataset_users, dataset_items, test_data, input_dim, latent_dim, num_epochs=30, batch_size=256, learning_rate=1e-3, mse_weight=8, l1_weight=0.3, kl_weight =0.003,  inner_product_weight = 1):\n",
        "    autoencoder = Autoencoder(latent_dim,input_dim, activation=nn.ReLU(), tied=True, normalize = True)\n",
        "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "    g = torch.Generator().manual_seed(42)\n",
        "    g_items = torch.Generator().manual_seed(42)\n",
        "\n",
        "    # # --- Create DataLoader for the user data\n",
        "    user_indices = torch.arange(dataset_users.shape[0]).unsqueeze(1)\n",
        "    dataset_users_wind = torch.cat((user_indices, dataset_users), dim=1)\n",
        "    # dataset_users_wind shape: [num_users, 1 + input_dim]\n",
        "    dataloader_users = DataLoader(dataset_users_wind, batch_size=batch_size, shuffle=True, drop_last=True, generator = g)\n",
        "\n",
        "    # Create DataLoader for the item data\n",
        "    item_indices = torch.tensor(df_item_emb_mf.index).unsqueeze(1)\n",
        "    dataset_items_wind = torch.cat((item_indices, dataset_items), dim=1)\n",
        "    dataloader_items = DataLoader(dataset_items_wind, batch_size=256, shuffle=True, drop_last=True, generator = g_items)\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        batch = 0\n",
        "        temp_activation_rates = []\n",
        "        print(f'epoch {epoch}')\n",
        "        n = 0\n",
        "        recon_sum = l1_sum = kl_sum = inner_sum = 0.0\n",
        "        if epoch !=0:\n",
        "          dataloader_items = DataLoader(dataset_items_wind, batch_size=256, shuffle=True, drop_last=True)\n",
        "        for data_users, data_items in zip(dataloader_users, itertools.cycle(dataloader_items)):\n",
        "\n",
        "            # # --------------------------\n",
        "            # # 1) Process user batch\n",
        "            # # --------------------------\n",
        "            # # data_users: shape [batch_size, 1 + input_dim]\n",
        "            user_idx  = data_users[:, 0].long()           # user indices, tensor\n",
        "            input_tensor_users = data_users[:, 1:]        # the actual features, tensor\n",
        "            normalized_users = normalize_matrix(input_tensor_users) # numpy.ndarray\n",
        "            input_tensor_users = pad_or_truncate_tensor(normalized_users, input_dim) # numpy.ndarray\n",
        "\n",
        "            latents_pre_act_usrs, latents_usrs, user_recons = autoencoder(input_tensor_users)\n",
        "\n",
        "            # --------------------------\n",
        "            # 2) Process item batch\n",
        "            # --------------------------\n",
        "            # data_items: shape [batch_size, 1 + input_dim]\n",
        "            item_idx = data_items[:, 0].long()\n",
        "            input_tensor_items = data_items[:, 1:]\n",
        "            normalized_items = normalize_matrix(input_tensor_items)\n",
        "            input_tensor_items = pad_or_truncate_tensor(normalized_items, input_dim)\n",
        "\n",
        "            latents_pre_act_items, latents_items, item_recons = autoencoder(input_tensor_items)\n",
        "\n",
        "            input_tensor_users = torch.tensor(input_tensor_users, dtype=torch.float32)\n",
        "            input_tensor_items = torch.tensor(input_tensor_items, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "            ## sparsity test:\n",
        "            activation_mask = (latents_items > 1e-6).float()\n",
        "            activation_rate = activation_mask.mean().item()\n",
        "            temp_activation_rates.append(activation_rate)\n",
        "            ##\n",
        "\n",
        "            ephochs_progress = epoch/num_epochs\n",
        "        \n",
        "            ##------------gradual prediction aware loss-----------------\n",
        "            # if we put ephochs_progress == -1, than the losss function will ignore gradual weights selection\n",
        "            # inner_product_weight = beta_schedule(epoch, num_epochs, beta_start=0.0, beta_end=1.0, warmup=5)\n",
        "            loss,reconstruction_loss, recon, l1c, klc, inner = autoencoder_loss(user_idx, input_tensor_users,item_idx, input_tensor_items,\n",
        "                    user_recons, item_recons,latents_usrs,latents_items,ephochs_progress,mse_weight=mse_weight,l1_weight=l1_weight, kl_weight =kl_weight,inner_product_weight = inner_product_weight\n",
        "                                    )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            recon_sum += recon.item(); l1_sum += l1c.item()\n",
        "            kl_sum += klc.item(); inner_sum += inner.item()\n",
        "            n += 1\n",
        "            batch+=1\n",
        "\n",
        "\n",
        "        # print(f\"Epoch {epoch}: recon={recon_sum/n:.4f} {reconstruction_loss} l1={l1_sum/n:.4f} kl={kl_sum/n:.4f} inner={inner_sum/n:.4f}\")\n",
        "\n",
        "        autoencoder.activation_rate[epoch] = temp_activation_rates\n",
        "        (autoencoder.loss).append(loss.item())\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return autoencoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgKcrVdqhC5h"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cdSn3x4Y1N_"
      },
      "outputs": [],
      "source": [
        "test_flag=1\n",
        "autoencoder=train_autoencoder(interaction_embeddings, dataset_items,dataset_users[test_subset_users], input_dim=dataset_users.shape[1], latent_dim=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsBCuCzThII5"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItN_uMtFX58u"
      },
      "outputs": [],
      "source": [
        "model_name = 'your_SAE_model_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB3z3PY6ZDWW"
      },
      "outputs": [],
      "source": [
        "# torch.save(autoencoder, Path(export_dir,f'models/lastFM/{model_name}.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhNrZ_HkHVlQ"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Q5XYZMnNCH"
      },
      "source": [
        "## SAE NCF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IYA6zYJoF_h"
      },
      "source": [
        "load NCF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKVKKnFGnkDE"
      },
      "outputs": [],
      "source": [
        "# Initialize the model.\n",
        "model = NeuralCollaborativeFiltering(num_users=USERS_lastFM, num_items=ITEMS_lastFM,\n",
        "                                      embedding_dim=100, hidden_layers=[64, 32, 16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "load existing model of NCF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV_AGc4ni7v_"
      },
      "outputs": [],
      "source": [
        "model = torch.load(Path(export_dir,'models/lastFM/NCF_recommender.pth'), weights_only=False)\n",
        "\n",
        "# pos_idx_ex_use = model.pos_idx_ex_use\n",
        "# pos_idx_ex_hidden = {(row): [item for item in pos_ex[row] if item not in pos_idx_ex_use[row]] for row in ratings_matrix.index}\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_embeddings = model.user_embedding.weight.detach().clone()  # Tensor of shape (num_users, embedding_dim)\n",
        "item_embeddings = model.item_embedding.weight.detach().clone() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_user_embeddings = pd.read_csv(Path(export_dir,'rec_csv/lastFM/ncf/users_embeddings_ncf_.csv'))\n",
        "user_embeddings = torch.tensor(df_user_embeddings.values, dtype=torch.float32)\n",
        "\n",
        "df_item_embeddings = pd.read_csv(Path(export_dir,'rec_csv/lastFM/ncf/items_embeddings_ncf_.csv'))\n",
        "item_embeddings = torch.tensor(df_item_embeddings.values, dtype=torch.float32)\n",
        "\n",
        "df_item_emb1 = df_item_embeddings.copy()\n",
        "df_item_emb1.index = user_artist_matrix.columns\n",
        "\n",
        "dataset_items = torch.tensor(df_item_embeddings.values, dtype=torch.float32)\n",
        "dataset_users = torch.tensor(df_user_embeddings.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koTf72ZroK1A"
      },
      "source": [
        "SAE training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw4eEubZnNCI"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 4. SAE Training Loop\n",
        "def train_sparse_autoencoder(model, sae_model, user_embed, item_embed, batch_size = 256, epochs=30, lr=0.001, device='cpu',mse_weight=2.6, sparsity_weight=1.2674255898937214e-05,kl_weight=0.7 , output_loss_weight = 1):\n",
        "    model.to(device)\n",
        "    sae_model.to(device)\n",
        "    optimizer = optim.Adam(sae_model.parameters(), lr=lr)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    # Set the fixed model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # for full determinism (might slow down training):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    g = torch.Generator().manual_seed(42)\n",
        "    g_items = torch.Generator().manual_seed(42)\n",
        "\n",
        "    user_indices = torch.arange(user_embed.shape[0]).unsqueeze(1)\n",
        "    user_indices = [ind for ind in user_indices if ind not in sae_model.test_subset_users_ind]\n",
        "    dataloader_users = DataLoader(user_indices, batch_size=batch_size, shuffle=True, drop_last=True,generator = g)\n",
        "\n",
        "    # - Create DataLoader for the item data\n",
        "    dataloader_items = DataLoader(range(item_embed.shape[0]), batch_size=batch_size, shuffle=True, drop_last=True, generator = g_items)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        total_loss = 0.0\n",
        "        batch = 0\n",
        "        samples_num = 0\n",
        "        # for user, item in dataloader:\n",
        "        # if epoch == 0:\n",
        "        for user, item in zip(dataloader_users, itertools.cycle(dataloader_items)):\n",
        "\n",
        "            user = torch.tensor(user, dtype=torch.long)\n",
        "            item = torch.tensor(item, dtype=torch.long)\n",
        "            user = user.squeeze()\n",
        "            item = item.squeeze()\n",
        "            real_ratings = user_artist_matrix_tensor[user, item].float()\n",
        "\n",
        "\n",
        "            # Forward pass through fixed model with original embeddings:\n",
        "            user_emb = user_embed[user]  \n",
        "            item_emb = item_embed[item]  \n",
        "            x_full = torch.cat([user_emb, item_emb], dim=-1)  # (batch, 200)\n",
        "\n",
        "            y = model.fc_layers(x_full)               # (batch, 1)\n",
        "            y = y.squeeze(-1)                           # (batch,)\n",
        "           \n",
        "            # ---------------------------\n",
        "            #  Forward pass through SAE for each embedding individually:\n",
        "            user_rec, user_encoded = sae_model(user_emb)  \n",
        "            item_rec, item_encoded = sae_model(item_emb)  \n",
        "          \n",
        "            # Compute reconstruction loss for each embedding.\n",
        "            loss_rec_user = mse_loss(user_rec, user_emb)\n",
        "            loss_rec_item = mse_loss(item_rec, item_emb)\n",
        "            loss_reconstruction = loss_rec_user + loss_rec_item\n",
        "\n",
        "            # Sparsity penalty (L1 norm of the encoded activations)\n",
        "            sparsity_loss_user = torch.mean(torch.abs(user_encoded))\n",
        "            sparsity_loss_item = torch.mean(torch.abs(item_encoded))\n",
        "            sparsity_loss = sparsity_loss_user + sparsity_loss_item\n",
        "\n",
        "            kl_loss_user = kl_divergence_loss(user_encoded)\n",
        "            kl_loss_item = kl_divergence_loss(item_encoded)\n",
        "            kl_loss =  (kl_loss_user + kl_loss_item)\n",
        "\n",
        "            # ---------------------------\n",
        "            # Pass reconstructed embeddings through fixed model:\n",
        "            x_hat = torch.cat([user_rec, item_rec], dim=-1)  \n",
        "            y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "            y_hat = y_hat.squeeze(-1)         # (batch,)\n",
        "\n",
        "\n",
        "            ce_loss = nn.BCEWithLogitsLoss() \n",
        "            loss_output = mse_loss(y_hat, y)\n",
        "            #------------------------------\n",
        "            recons_NCF_loss = ce_loss(y_hat, real_ratings)\n",
        "            #------------------------------\n",
        "\n",
        "            # Total loss: reconstruction loss + prediction aware loss + sparsity penalty.\n",
        "            loss = mse_weight*loss_reconstruction + output_loss_weight*loss_output + sparsity_weight * sparsity_loss + kl_weight*kl_loss + 0*recons_NCF_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # Gradients flow from the fixed network (frozen) through SAE.\n",
        "            optimizer.step()\n",
        "            samples_num += len(real_ratings)\n",
        "\n",
        "            total_loss += loss.item() *  len(real_ratings)\n",
        "\n",
        "\n",
        "        avg_loss = total_loss / samples_num\n",
        "        (sae_model.loss).append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpllRnzloRwP"
      },
      "outputs": [],
      "source": [
        "test_flag=1\n",
        "sae_model = SparseAutoencoderNCF(input_dim=100, hidden_dim=70, topk=7, tie_weights=True)\n",
        "train_sparse_autoencoder(model, sae_model, user_embeddings, item_embeddings, epochs=30, lr=0.001, device='cpu',mse_weight=2.6, sparsity_weight=1.2674255898937214e-05,kl_weight=0.7, output_loss_weight = 10)\n",
        "sae_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P32GUZmarGGY"
      },
      "outputs": [],
      "source": [
        "model_name = 'your_SAE_model_name'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ak8u1mJrH-d"
      },
      "outputs": [],
      "source": [
        "# torch.save(autoencoder, Path(export_dir,f'models/lastFM/{model_name}.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAE Matryoshka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_matryoshka(\n",
        "    sae: MatryoshkaAutoencoder,\n",
        "    user_loader: DataLoader,\n",
        "    item_loader: DataLoader,\n",
        "    num_epochs: int = 30,\n",
        "    lr: float = 1e-3,\n",
        "    mse_weight: float = 8.0,\n",
        "    l1_weight: float = 0.3,\n",
        "    kl_weight: float = 0.003,\n",
        "    inner_weight: float = 5.0,\n",
        "    sparsity_target: float = 0.1,\n",
        "    device: str = 'cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    trains MatryoshkaSAEs on user and item embeddings.\n",
        "    \"\"\"\n",
        "    # sae.to(device)\n",
        "\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # for full determinism (might slow down training):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    optimizer = torch.optim.Adam(sae.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        # Iterate in parallel over user and item batches\n",
        "        for u,i in zip(user_loader, item_loader):\n",
        "            # u = batch_u.to(device)  # user MF embeddings\n",
        "            # i = batch_i.to(device)  # item MF embeddings\n",
        "\n",
        "            # Forward pass through autoencoder\n",
        "            _, z_u, recons_u = sae(u)\n",
        "            _, z_i, recons_i = sae(i)\n",
        "\n",
        "\n",
        "            # 1) Nested reconstruction loss (sum over all prefix levels)\n",
        "            recon_u_loss = sum(F.mse_loss(r, u) for r in recons_u)\n",
        "            recon_i_loss = sum(F.mse_loss(r, i) for r in recons_i)\n",
        "            recon_term = mse_weight * (recon_u_loss + recon_i_loss)\n",
        "\n",
        "            # 2) Sparsity penalties on full latent vectors- z\n",
        "            l1_term = l1_weight * (z_u.abs().mean() + z_i.abs().mean())\n",
        "            kl_term = kl_weight * (\n",
        "                kl_divergence_loss(z_u, sparsity_target) +\n",
        "                kl_divergence_loss(z_i, sparsity_target)\n",
        "            )\n",
        "\n",
        "            # 3) Dynamic inner-product loss on full reconstructions\n",
        "            full_u_hat = recons_u[-1]  # highest-level reconstruction\n",
        "            full_i_hat = recons_i[-1]\n",
        "            inner_orig = u @ i.T\n",
        "            inner_recons = full_u_hat @ full_i_hat.T\n",
        "\n",
        "            inner_term = inner_weight * F.mse_loss(inner_recons, inner_orig)\n",
        "\n",
        "            # Total loss\n",
        "            loss = recon_term + l1_term + kl_term + inner_term\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * u.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(user_loader.dataset)\n",
        "        print(f\"Epoch {epoch}/{num_epochs} â€” avg_loss: {avg_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spCRSwsQHYjX"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyLso2Tm9PL"
      },
      "source": [
        "## MF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xlySSz9wZV0"
      },
      "outputs": [],
      "source": [
        "def train_gpu(self, batch_size=8, save_batches=False, batches_save_path=None):\n",
        "    self.batch_history = []\n",
        "    # self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.P = torch.tensor(self.P, dtype=torch.float32)\n",
        "    self.Q = torch.tensor(self.Q, dtype=torch.float32)\n",
        "    self.b_u = torch.tensor(self.b_u, dtype=torch.float32)\n",
        "    self.b_i = torch.tensor(self.b_i, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(self.iterations):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Sampling\n",
        "        neg_idx_ex_use__ = {\n",
        "            row: torch.tensor(\n",
        "                np.random.choice(self.neg_ex_use[row],\n",
        "                                 size=len(self.pos_idx_ex_use[row]),\n",
        "                                 replace=False,\n",
        "                                 p=norm_prob_neg_use_exp_[row]),\n",
        "                dtype=torch.long\n",
        "            ) for row in range(self.num_users)\n",
        "        }\n",
        "\n",
        "\n",
        "        # Building unified tensor of (user, item, label)\n",
        "        user_ids, item_ids, labels = [], [], []\n",
        "        for user in range(self.num_users):\n",
        "            user_tensor = torch.full((len(pos_idx_ex_use_[user]) + len(neg_idx_ex_use__[user]),),\n",
        "                                     user, dtype=torch.long)\n",
        "\n",
        "            item_tensor = torch.cat([pos_idx_ex_use_[user], neg_idx_ex_use__[user]])\n",
        "            label_tensor = torch.cat([\n",
        "                torch.ones(len(pos_idx_ex_use_[user])),\n",
        "                torch.zeros(len(neg_idx_ex_use__[user]))\n",
        "            ])\n",
        "            user_ids.append(user_tensor)\n",
        "            item_ids.append(item_tensor)\n",
        "            labels.append(label_tensor)\n",
        "\n",
        "        user_ids = torch.cat(user_ids)\n",
        "        item_ids = torch.cat(item_ids)\n",
        "        labels = torch.cat(labels)\n",
        "\n",
        "        # Shuffle all samples\n",
        "        indices = torch.randperm(len(user_ids))\n",
        "        user_ids = user_ids[indices]\n",
        "        item_ids = item_ids[indices]\n",
        "        labels = labels[indices]\n",
        "\n",
        "        if save_batches:\n",
        "            epoch_batches = []\n",
        "\n",
        "        # Training in mini-batches\n",
        "        for start in range(0, len(user_ids), batch_size):\n",
        "            end = start + batch_size\n",
        "            u_batch = user_ids[start:end]\n",
        "            i_batch = item_ids[start:end]\n",
        "            r_batch = labels[start:end]\n",
        "\n",
        "            u_batch = u_batch.long()\n",
        "            i_batch = i_batch.long()\n",
        "\n",
        "            self.sgd_batch_tensor(u_batch, i_batch, r_batch)\n",
        "\n",
        "            if save_batches:\n",
        "                epoch_batches.append((u_batch.cpu(), i_batch.cpu(), r_batch.cpu()))\n",
        "\n",
        "        if save_batches:\n",
        "            self.batch_history.append(epoch_batches)\n",
        "\n",
        "        rmse_val = self.rmse_tensor(user_ids, item_ids, labels)\n",
        "        self.rmse_train[epoch] = rmse_val\n",
        "        print(f\"Epoch {epoch+1}; RMSE: {rmse_val:.4f}; epoch time: {time.time()-start_time:.2f}s\")\n",
        "\n",
        "    if save_batches and batches_save_path:\n",
        "        with open(Path(export_dir, f'dataset/lastFM/{batches_save_path}'), 'wb') as f:\n",
        "            pickle.dump(self.batch_history, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDzYY1-HapC"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train new model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cslxnp6udDk"
      },
      "outputs": [],
      "source": [
        "mf_recommender = MatrixFactorization(user_artist_matrix_tensor, user_artist_matrix, pos_idx_ex_use_,neg_idx_ex_use_,neg_ex_hidden_, neg_ex_, pos_ex_num_, K=100, alpha=0.05, beta=0.01, iterations=30)\n",
        "\n",
        "start_time = time.time()\n",
        "train_gpu(mf_recommender, batch_size=8)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'your_recommender_name'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(Path(export_dir,'models/lastFM', f'{model_name}.pkl'), 'wb') as file:\n",
        "#     pickle.dump(mf_recommender, file)\n",
        "\n",
        "\n",
        "# df_Q= pd.DataFrame(mf_recommender.Q)\n",
        "# df_Q.index = user_artist_matrix.columns\n",
        "\n",
        "# df_Q.to_csv(Path(export_dir,f'res_csv/lastFM/items_embeddings_mf_model_{model_name}_Windex.csv'))\n",
        "# df_Q.to_csv(Path(export_dir,'res_csv/lastFM/items_embeddings_mf_model_{model_name}.csv'), index=False)\n",
        "\n",
        "\n",
        "# df_P=  pd.DataFrame(mf_recommender.P)\n",
        "# df_P.index = user_artist_matrix.index\n",
        "# df_P.to_csv(Path(export_dir,f'res_csv/lastFM/users_embeddings_mf_model_{model_name}.csv'), index=False)\n",
        "# df_P.to_csv(Path(export_dir,f'res_csv/lastFM/users_embeddings_mf_model_{model_name}_Windex.csv'))\n",
        "\n",
        "# df_b_i= pd.DataFrame(mf_recommender.b_i) \n",
        "# df_b_u= pd.DataFrame(mf_recommender.b_u) \n",
        "\n",
        "# df_b_i.index = user_artist_matrix.columns\n",
        "# df_b_u.index = user_artist_matrix.index\n",
        "\n",
        "# df_b_i.to_csv(Path(export_dir,f'res_csv/lastFM/b_i_mf_model_{model_name}.csv'), index=False)\n",
        "# df_b_i.to_csv(Path(export_dir,f'res_csv/lastFM/b_i_mf_model_{model_name}_Windex.csv'))\n",
        "\n",
        "# df_b_u.to_csv(Path(export_dir,f'res_csv/lastFM/b_u_mf_model_{model_name}.csv'), index=False)\n",
        "# df_b_u.to_csv(Path(export_dir,f'res_csv/lastFM/b_u_mf_model_{model_name}_Windex.csv'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agHtftqgznEQ"
      },
      "outputs": [],
      "source": [
        "# with open(Path(export_dir,f'models/lastFM/MF_model_{model_name}'), 'wb') as file:\n",
        "#     pickle.dump(mf_recommender, file)\n",
        "\n",
        "\n",
        "df_b_i= pd.DataFrame(mf_recommender.b_i)\n",
        "df_b_u= pd.DataFrame(mf_recommender.b_u)\n",
        "df_b_i.index = user_artist_matrix.columns\n",
        "df_b_u.index = user_artist_matrix.index\n",
        "\n",
        "\n",
        "df_b_i.to_csv(Path(export_dir,f'res_csv/lastFM/b_i_{model_name}.csv'), index=False)\n",
        "# df_b_i.to_csv(Path(export_dir,f'res_csv/b_i_{model_name}_Windex.csv'))\n",
        "\n",
        "df_b_u.to_csv(Path(export_dir,f'res_csv/lastFM/b_u_{model_name}.csv'), index=False)\n",
        "# df_b_u.to_csv(Path(export_dir,f'res_csv/b_u_{model_name}_Windex.csv'))\n",
        "\n",
        "pos_idx_ex_hidden_df= pd.DataFrame.from_dict(mf_recommender.pos_idx_ex_hidden, orient='index')\n",
        "pos_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/lastFM/test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "neg_idx_ex_hidden_df= pd.DataFrame.from_dict(mf_recommender.neg_ex_hidden, orient='index')\n",
        "neg_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/lastFM/neg_test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "\n",
        "df_P= pd.DataFrame(mf_recommender.P)\n",
        "df_P.to_csv(Path(export_dir,f'res_csv/lastFM/lastFM/users_embeddings_{model_name}.csv'), index = False)\n",
        "\n",
        "\n",
        "df_Q= pd.DataFrame(mf_recommender.Q)\n",
        "df_Q.to_csv(Path(export_dir,f'res_csv/lastFM/items_embeddings_{model_name}.csv'), index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxqfClGXHaxc"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4FJSbJ1Hazn"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjgkFFLBm-lg"
      },
      "source": [
        "## NCF recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywa8PHXznMSU"
      },
      "outputs": [],
      "source": [
        "# Training loop for the model.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def train_model(model, ratings, epochs=30, lr=0.001, device='cpu'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss() \n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        counter = 0\n",
        "        epoch_loss = 0.0\n",
        "        start_time1 = time.time()\n",
        "\n",
        "\n",
        "        #  Sampling\n",
        "        neg_idx_ex_use__ = {\n",
        "            row: torch.tensor(\n",
        "                np.random.choice(user_artist_matrix.columns.get_indexer(model.neg_ex_use[row]),\n",
        "                                 size=len(model.pos_idx_ex_use[row]),\n",
        "                                 replace=False,\n",
        "                                 p=norm_prob_neg_use_exp_[row]),\n",
        "                dtype=torch.long\n",
        "            ) for row in range(model.num_users)\n",
        "        }\n",
        "\n",
        "        #  Build unified tensor of (user, item, label)\n",
        "        user_ids, item_ids, labels = [], [], []\n",
        "        for user in range(model.num_users):\n",
        "            user_tensor = torch.full((len(pos_idx_ex_use_[user]) + len(neg_idx_ex_use__[user]),),\n",
        "                                     user, dtype=torch.long)\n",
        "            item_tensor = torch.cat([torch.Tensor(list(user_artist_matrix.columns.get_indexer(pos_idx_ex_use_[user]))), neg_idx_ex_use__[user]])\n",
        "\n",
        "            label_tensor = torch.cat([\n",
        "                torch.ones(len(pos_idx_ex_use_[user]), dtype=torch.float32),\n",
        "                torch.zeros(len(neg_idx_ex_use__[user]), dtype=torch.float32)\n",
        "            ])\n",
        "            user_ids.append(user_tensor)\n",
        "            item_ids.append(item_tensor)\n",
        "            labels.append(label_tensor)\n",
        "\n",
        "        user_ids = torch.cat(user_ids)\n",
        "        item_ids = torch.cat(item_ids)\n",
        "        labels = torch.cat(labels)\n",
        "\n",
        "        # Shuffle all samples\n",
        "        indices = torch.randperm(len(user_ids))\n",
        "        user_ids = user_ids[indices]\n",
        "        item_ids = item_ids[indices]\n",
        "        labels = labels[indices]\n",
        "\n",
        "        # if save_batches:\n",
        "        #     epoch_batches = []\n",
        "\n",
        "        # print('print batch')\n",
        "        #Training in mini-batches\n",
        "        for start in range(0, len(user_ids), batch_size):\n",
        "            end = start + batch_size\n",
        "            user = user_ids[start:end] # u_batch\n",
        "            item = item_ids[start:end] # i_batch\n",
        "            rating = labels[start:end] # r_batch\n",
        "\n",
        "            user = user.long()\n",
        "            item = item.long()\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(user, item)\n",
        "            prediction = prediction.view_as(rating)\n",
        "            # print(prediction)\n",
        "            loss = criterion(prediction, rating)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * user.size(0)\n",
        "            counter +=1\n",
        "        \n",
        "\n",
        "\n",
        "        avg_loss = epoch_loss / counter # num of batches\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, time: {time.time() - start_time1}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJZ79231stWl"
      },
      "source": [
        "trainings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaXhAa3x7Lwt"
      },
      "outputs": [],
      "source": [
        "# Initialize the model.\n",
        "model = NeuralCollaborativeFiltering(num_users=USERS_lastFM, num_items=ITEMS_lastFM,\n",
        "                                      embedding_dim=100, hidden_layers=[64, 32, 16])\n",
        "# Train the model.\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "train_model(model, user_artist_matrix_tensor)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AG5BuV7-5ay"
      },
      "outputs": [],
      "source": [
        "model_name = 'your NCF recommender name'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.save(model, Path(export_dir,f'models/lastFM/{model_name}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd1J8_Tn9otP"
      },
      "outputs": [],
      "source": [
        "# user_embeddings = model.user_embedding.weight.detach().clone()  # Tensor of shape (num_users, embedding_dim)\n",
        "# item_embeddings = model.item_embedding.weight.detach().clone()  # Tensor of shape (num_items, embedding_dim)\n",
        "\n",
        "\n",
        "# pos_idx_ex_hidden_df= pd.DataFrame.from_dict(model.pos_idx_ex_hidden, orient='index')\n",
        "# pos_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/lastFM/test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "# neg_idx_ex_hidden_df= pd.DataFrame.from_dict(model.neg_ex_hidden, orient='index')\n",
        "# neg_idx_ex_hidden_df.to_csv(Path(export_dir,f'res_csv/lastFM/neg_test_items_{model_name}.csv'), index = False)\n",
        "\n",
        "\n",
        "# user_embeddings.to_csv(Path(export_dir,f'res_csv/lastFM/NCF_{model_name}_user_embeddings.csv'), index = False)\n",
        "# item_embeddings.to_csv(Path(export_dir,f'res_csv/lastFM/NCF_{model_name}_item_embeddings.csv'), index = False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
