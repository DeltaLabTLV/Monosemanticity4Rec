{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxb-UtzcjhDq"
      },
      "source": [
        "## This code contains help functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-jU-KhY6LHq"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "export_dir = os.getcwd()\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import plot\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "from scipy.special import expit  # Sigmoid function\n",
        "import itertools\n",
        "from IPython.display import Latex, display\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Ignore FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "test_flag = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rvEWZkEhWte"
      },
      "outputs": [],
      "source": [
        "pip install ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1CmF7q1h6iF"
      },
      "outputs": [],
      "source": [
        "from ipynb.fs.defs.data_processing import *\n",
        "from ipynb.fs.defs.models import *\n",
        "from ipynb.fs.defs.training import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## prediction_aware_loss == output_loss == inner_product_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG9CmIrv1Erl"
      },
      "source": [
        "Test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSOcYxWo1Eca"
      },
      "outputs": [],
      "source": [
        "# from data processing\n",
        "def test_set_gen(df_recommender_user_emb=df_user_mf, df_recommender_item_emb=df_item_mf):\n",
        "  test_subset_users = random.sample(list(df_recommender_user_emb.index), k=math.floor(df_recommender_user_emb.shape[0]*0.2))\n",
        "  test_subset_items = random.sample(list(df_recommender_item_emb.index), k=math.floor(df_recommender_item_emb.shape[0]*0.2))\n",
        "\n",
        "  return test_subset_users, test_subset_items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuxN7L5qpalN"
      },
      "source": [
        "Load models for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTKOKHo_2Wbk"
      },
      "outputs": [],
      "source": [
        "# from model intialization- init test sets for users and items:\n",
        "test_subset_users, test_subset_items = test_set_gen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VIV2gXCiSxC"
      },
      "outputs": [],
      "source": [
        "#mf SAE\n",
        "test_flag=1\n",
        "autoencoder = Autoencoder(latent_dim, input_dim, activation=nn.ReLU(), tied=True, normalize = True)\n",
        "train_autoencoder(interaction_embeddings, dataset_items,dataset_users[test_subset_users],input_dim=dataset_users.shape[1], latent_dim=22)\n",
        "\n",
        "# NCF SAE\n",
        "sae_model = SparseAutoencoderNCF(input_dim=100, hidden_dim=70, topk=7, tie_weights=True)\n",
        "\n",
        "# Matryoshka SAE\n",
        "K = 80\n",
        "prefixes = [K // 4, K // 2, K]\n",
        "sae_shared = MatryoshkaAutoencoder(latent_dim=K, input_dim=100, group_sizes=prefixes)\n",
        "\n",
        "# MF\n",
        "mf_recommender = MatrixFactorization(user_artist_matrix_tensor, pos_idx_ex_use,neg_idx_ex_use,neg_ex_hidden, neg_ex, pos_ex_num, K=22, alpha=0.05, beta=0.01, iterations=6, pop_flag = 1)\n",
        "\n",
        "# NCF\n",
        "model = NeuralCollaborativeFiltering(num_users=22546, num_items=2277,\n",
        "                                      embedding_dim=100, hidden_layers=[64, 32, 16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umkpSmOBYnya"
      },
      "source": [
        "# KL loss term implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kl_divergence_loss(latent_activations, sparsity_target=0.05, eps=1e-6):\n",
        "\n",
        "    # Calculate average activation of each latent unit and clamp to avoid exact 0 or 1\n",
        "    rho_hat = torch.clamp(torch.mean(latent_activations, dim=0), eps, 1 - eps)\n",
        "\n",
        "    # Define the target sparsity; assumed to be within (0,1)\n",
        "    rho = torch.tensor(sparsity_target, dtype=torch.float32, device=latent_activations.device)\n",
        "\n",
        "    # Compute the KL divergence with eps added inside the log to ensure numerical stability\n",
        "    kl_div = rho * torch.log((rho + eps) / (rho_hat + eps)) + \\\n",
        "             (1 - rho) * torch.log(((1 - rho) + eps) / ((1 - rho_hat) + eps))\n",
        "\n",
        "    # Return the sum over all latent units\n",
        "    return torch.sum(kl_div)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Jg2qM2GLqw"
      },
      "outputs": [],
      "source": [
        "def LN(x: torch.Tensor, eps: float = 1e-5) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    if type(x) == np.ndarray:\n",
        "      x= torch.from_numpy(x)\n",
        "    mu = x.mean(dim=-1, keepdim=True)\n",
        "    x = x - mu\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    x = x / (std + eps)\n",
        "    return x, mu, std\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "        x, mu, std = LN(x)\n",
        "        return x, dict(mu=mu, std=std)\n",
        "\n",
        "\n",
        "def beta_schedule(epoch: int, max_epochs: int, beta_start: float = 0.0, beta_end: float = 10.0, warmup: int = 13) -> float:\n",
        "    \"\"\"\n",
        "    Returns a β value that is small initially (for `warmup` epochs) and then\n",
        "    grows linearly from beta_start to beta_end over the remaining epochs.\n",
        "\n",
        "    Args:\n",
        "        epoch (int): current epoch (0-based).\n",
        "        max_epochs (int): total number of epochs.\n",
        "        beta_start (float): starting β value.\n",
        "        beta_end (float): final β value.\n",
        "        warmup (int): number of epochs to hold at beta_start.\n",
        "    \"\"\"\n",
        "    if epoch < warmup:\n",
        "        return beta_start\n",
        "    # linearly ramp from epoch=warmup -> epoch=max_epochs\n",
        "    progress = (epoch - warmup) / max(1, (max_epochs - warmup))\n",
        "    return beta_start + progress * (beta_end - beta_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pkH3EheXCrb"
      },
      "source": [
        "#Generating result table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2-mT_2ZumkZ"
      },
      "outputs": [],
      "source": [
        "'''object means a list of indexes not names of the artists. df_tags.index is the numbers of the artists themselves\n",
        "num_users_per_artist_sort, main_data, main_data_names are  dataframe, '''\n",
        "\n",
        "def table_maker_new(table_size:int, objects:list, main_data=df_tags, pop_ranking=num_users_per_artist_sort):\n",
        "    table = pd.DataFrame(0, index=range(table_size), columns=['Name', 'artist ID', 'Genre'])\n",
        "    for i in table.index:\n",
        "      table.iloc[i,0] =  main_data.index[objects[i]]\n",
        "      table.iloc[i,1] =  objects[i]\n",
        "      table.iloc[i,2] =  ', '.join(main_data.iloc[objects[i]][0])\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErW9OirLAEr_"
      },
      "source": [
        "# Generating group of users w.r.t a certain concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6AsSww-71R2"
      },
      "outputs": [],
      "source": [
        "'''find the group'''\n",
        "'''list of users that listened to artists of certain genre, sorted by the number of\n",
        "artists of the certain genre wrt the total number of artists ranked by the user'''\n",
        "\n",
        "\n",
        "def generate_users_test_group(group_concept, N,model_name):\n",
        "\n",
        "  'generates group of users who have cetain dominant preferences of specific concept'\n",
        "\n",
        "  concept_ids = np.where(df_artists_tags[group_concept] == 1)\n",
        "\n",
        "  group_concept_artists = np.array(df_artists_tags.iloc[concept_ids].index)\n",
        "  part_sum_for_group_concept=user_artist_matrix.loc[:,list(group_concept_artists)].sum(axis=1)/user_artist_matrix.sum(axis=1)\n",
        "  argmax_user = part_sum_for_group_concept.iloc[model_name.test_subset_users_ind].nlargest(N).index # N users listened the biggest amount of 'genre' artists movies vs the total num of artists\n",
        "\n",
        "  # users listened most of group_concept artists, the number of group_concept artists\n",
        "  # they listened, and the number of artists they watched from all concepts\n",
        "  usersGroup=pd.DataFrame(part_sum_for_group_concept.loc[argmax_user])\n",
        "  usersGroup.columns = [f\"percentage of {group_concept} movies\"]\n",
        "\n",
        "  return usersGroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TopK Recommendations Functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_pq(user_id1, p:torch.tensor, q: torch.tensor, num):\n",
        "  with torch.no_grad():\n",
        "    sorted_user_recommedations_pq = pd.DataFrame((p@q.T).detach().cpu().numpy()).iloc[user_id1,:].sort_values(ascending = False)\n",
        "    pq_recommendations_analysis_per_user_unknown =sorted_user_recommedations_pq.drop(index=np.where(user_artist_matrix.iloc[user,:]==1)[0])\n",
        "\n",
        "    sorted_user_recommedations_pq_ind = pq_recommendations_analysis_per_user_unknown.index\n",
        "    sorted_user_recommedations_pq_name = list(user_artist_matrix.columns[pq_recommendations_analysis_per_user_unknown.index])\n",
        "    top_rec_user_id = list(sorted_user_recommedations_pq_ind[0:num-1])\n",
        "\n",
        "  return top_rec_user_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_pq_all(user_id1, p:torch.tensor, q: torch.tensor, num):\n",
        "\n",
        "  sorted_user_recommedations_pq = pd.DataFrame(p@q.T).iloc[user_id1,:].sort_values(ascending = False)\n",
        "#   scores = (p @ q.T).detach().cpu().numpy()\n",
        "#   sorted_user_recommedations_pq = pd.Series(scores[user_id1]).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "  sorted_user_recommedations_pq_ind = sorted_user_recommedations_pq.index\n",
        "  sorted_user_recommedations_pq_name = list(user_artist_matrix.columns[sorted_user_recommedations_pq.index])\n",
        "  top_rec_user_id = list(sorted_user_recommedations_pq_ind[0:num])\n",
        "\n",
        "  return top_rec_user_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Urwu9vpxLK"
      },
      "source": [
        "# MF help functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le1SyoMa7T7S"
      },
      "outputs": [],
      "source": [
        "def normalize_matrix(matrix):\n",
        "    matrix = matrix.float()\n",
        "    min_val,_ = matrix.min(axis=0)\n",
        "    max_val,_ = matrix.max(axis=0)\n",
        "    normalized_matrix = (matrix - min_val) / (max_val - min_val)\n",
        "    return np.nan_to_num(normalized_matrix)\n",
        "\n",
        "#Converts sparse input to dense tensor.\n",
        "def convert_to_dense_tensor(sparse_matrix):\n",
        "    dense_matrix = np.array(sparse_matrix)\n",
        "    dense_tensor = torch.tensor(dense_matrix, dtype=torch.float32)\n",
        "    return dense_tensor\n",
        "\n",
        "# Ensures uniform input dimensions.\n",
        "def pad_or_truncate_tensor(tensor1, target_dim):\n",
        "    flattened_tensor = tensor1.reshape(tensor1.shape[0], -1)\n",
        "    if flattened_tensor.shape[1] < target_dim:\n",
        "        padded_tensor = torch.nn.functional.pad(flattened_tensor, (0, target_dim - flattened_tensor.size(1)))\n",
        "    else:\n",
        "        padded_tensor = flattened_tensor[:, :target_dim]\n",
        "    return padded_tensor\n",
        "\n",
        "# Ensures uniform input dimensions.\n",
        "def pad_or_truncate_tensor_0(tensor1, target_dim):\n",
        "    flattened_tensor = tensor1.reshape(tensor1.shape[0], -1)\n",
        "    if flattened_tensor.shape[0] < target_dim:\n",
        "        padded_tensor = torch.nn.functional.pad(flattened_tensor, (0, target_dim - flattened_tensor.size(1)))\n",
        "    else:\n",
        "        padded_tensor = flattened_tensor[:, :target_dim]\n",
        "    return padded_tensor\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lBknBkJ8H0"
      },
      "source": [
        "# Lists Correlation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMAB6VkAJqx4"
      },
      "outputs": [],
      "source": [
        "def rbo(list1, list2, p=0.9):\n",
        "    \"\"\"\n",
        "    Calculate Rank Biased Overlap (RBO) between two ranked lists.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first ranked list.\n",
        "        list2 (list): The second ranked list.\n",
        "        p (float): The probability of considering ranks deeper in the list (default=0.9).\n",
        "                   Higher values give more weight to deeper ranks.\n",
        "\n",
        "    Returns:\n",
        "        float: The RBO score between the two lists.\n",
        "    \"\"\"\n",
        "    # Lengths of the two lists\n",
        "    len1, len2 = len(list1), len(list2)\n",
        "    max_depth = max(len1, len2)\n",
        "\n",
        "    # Track cumulative overlap\n",
        "    cumulative_overlap = 0\n",
        "    agreement = 0  # Overlap count at each depth\n",
        "\n",
        "    for d in range(1, max_depth + 1):\n",
        "        # Get the top-d elements from both lists\n",
        "        top_d1 = set(list1[:d])\n",
        "        top_d2 = set(list2[:d])\n",
        "\n",
        "        # Calculate overlap at depth d\n",
        "        agreement = len(top_d1.intersection(top_d2))\n",
        "\n",
        "        # Weighted contribution to RBO\n",
        "        cumulative_overlap += (p ** (d - 1)) * (agreement / d)\n",
        "\n",
        "    # RBO formula\n",
        "    rbo_score = (1 - p) * cumulative_overlap\n",
        "    return rbo_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36nVEkPhFz-Z"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import kendalltau\n",
        "\n",
        "def kendall_tau(list1, list2):\n",
        "    \"\"\"\n",
        "    Calculate Kendall Tau correlation between two ranked lists.\n",
        "\n",
        "    Args:\n",
        "        list1 (list): The first ranked list.\n",
        "        list2 (list): The second ranked list.\n",
        "\n",
        "    Returns:\n",
        "        float: Kendall Tau correlation coefficient.\n",
        "    \"\"\"\n",
        "    # Create ranking dictionaries for both lists\n",
        "    rank1 = {item: rank for rank, item in enumerate(list1, 1)}\n",
        "    rank2 = {item: rank for rank, item in enumerate(list2, 1)}\n",
        "\n",
        "    # Make a union of all elements\n",
        "    all_items = list(set(list1) | set(list2))\n",
        "\n",
        "    # Convert ranks to aligned lists (fill missing elements with default ranks)\n",
        "    aligned_rank1 = [rank1.get(item, len(list1) + 1) for item in all_items]\n",
        "    aligned_rank2 = [rank2.get(item, len(list2) + 1) for item in all_items]\n",
        "\n",
        "    # Use scipy's kendalltau for calculation\n",
        "    tau, _ = kendalltau(aligned_rank1, aligned_rank2)\n",
        "    return tau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monosemanticity Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ms_score_new(df_cosine_sim_matrix, latents_items):\n",
        "  A = latents_items.detach().cpu().numpy()\n",
        "  scaler = MinMaxScaler()\n",
        "  A_norm = scaler.fit_transform(A)\n",
        "\n",
        "  N = A.shape[0]\n",
        "  K=30 \n",
        "\n",
        "  MS_scores_topK = {}\n",
        "\n",
        "  for k in range(latents_items.shape[1]):  # for each neuron\n",
        "      if k in range(latents_items.shape[1]):\n",
        "        a_k = A_norm[:, k]\n",
        "\n",
        "        top_k_idx = np.argsort(a_k)[-K:]\n",
        "        top_k_vals = a_k[top_k_idx]\n",
        "\n",
        "        # outer productof activation matrix\n",
        "        R_k = np.outer(top_k_vals, top_k_vals)\n",
        "        np.fill_diagonal(R_k, 0)\n",
        "\n",
        "        # similarity matrix\n",
        "        df_cosine_sim_matrix_array = df_cosine_sim_matrix.values\n",
        "        S_top_k = df_cosine_sim_matrix_array[np.ix_(top_k_idx, top_k_idx)]\n",
        "        np.fill_diagonal(S_top_k, 0)\n",
        "\n",
        "        # Calculate MS\n",
        "        denom = np.sum(R_k)\n",
        "        MS_k = np.sum(R_k * S_top_k) / denom if denom != 0 else 0\n",
        "        MS_scores_topK[k] = MS_k\n",
        "\n",
        "  return sum(MS_scores_topK.values())/latents_items.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ms_score_all_neurons(df_cosine_sim_matrix, latents_items):\n",
        "  A = latents_items.detach().cpu().numpy()\n",
        "  scaler = MinMaxScaler()\n",
        "  A_norm = scaler.fit_transform(A)\n",
        "\n",
        "  N = A.shape[0]\n",
        "  K=30 \n",
        "\n",
        "  MS_scores_topK = {}\n",
        "\n",
        "  for k in range(latents_items.shape[1]):  # for each neuron\n",
        "      if k in range(latents_items.shape[1]):\n",
        "        a_k = A_norm[:, k]\n",
        "\n",
        "        top_k_idx = np.argsort(a_k)[-K:]\n",
        "        top_k_vals = a_k[top_k_idx]\n",
        "\n",
        "        # outer productof activation matrix\n",
        "        R_k = np.outer(top_k_vals, top_k_vals)\n",
        "        np.fill_diagonal(R_k, 0)\n",
        "\n",
        "        # similarity matrix\n",
        "        df_cosine_sim_matrix_array = df_cosine_sim_matrix.values\n",
        "        S_top_k = df_cosine_sim_matrix_array[np.ix_(top_k_idx, top_k_idx)]\n",
        "        np.fill_diagonal(S_top_k, 0)\n",
        "\n",
        "        # Calculate MS\n",
        "        denom = np.sum(R_k)\n",
        "        MS_k = np.sum(R_k * S_top_k) / denom if denom != 0 else 0\n",
        "        MS_scores_topK[k] = MS_k\n",
        "\n",
        "  return MS_scores_topK.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI6PqhmtJrTN"
      },
      "source": [
        "# Recommendations Extraction- NCF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBcayidm3l1a"
      },
      "outputs": [],
      "source": [
        "def get_top_k_recommendations(model, user_id, candidate_item_ids, K):\n",
        "    \"\"\"\n",
        "    Returns the top K recommended item IDs for a given user.\n",
        "\n",
        "    Args:\n",
        "        model: Trained NeuralCollaborativeFiltering model.\n",
        "        user_id: The user ID for which recommendations are needed.\n",
        "        candidate_item_ids: List (or tensor) of candidate item IDs.\n",
        "        K: The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top K recommended item IDs.\n",
        "    \"\"\"\n",
        "    # Create a tensor for the user repeated for each candidate item.\n",
        "    user_tensor = torch.tensor([user_id] * len(candidate_item_ids), dtype=torch.long)\n",
        "    item_tensor = torch.tensor(candidate_item_ids, dtype=torch.long)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get predictions for all candidate items.\n",
        "        scores  = model(user_tensor, item_tensor)- \\\n",
        "         - model.user_bias(user_tensor).squeeze(-1) \\\n",
        "         - model.item_bias(item_tensor).squeeze(-1)\n",
        "\n",
        "    sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "    # Select the top K item IDs.\n",
        "    top_k_indices = sorted_indices[:K+40]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "\n",
        "    recommendations = [(user_artist_matrix.columns[top_k_item_ids[i]], round(float(scores[top_k_item_ids[i]].data),6)) for i in range(K+40)]\n",
        "\n",
        "    artist_recommendations = [artistt[0] for artistt in recommendations] #pd.DataFrame(df_user_emb@(df_item_emb.T)).iloc[user_id,:].sort_values(ascending = False)\n",
        "    artist_recommendations = merged_df__all.loc[artist_recommendations,:]\n",
        "\n",
        "    artist_recommendations = artist_recommendations.loc[list(set(artist_recommendations.index)- set(user_artist_matrix.columns[np.where(user_artist_matrix.iloc[user_id,:]==1)[0]])),:]\n",
        "    top_k_item_ids = user_artist_matrix.columns.get_indexer(artist_recommendations.index)\n",
        "\n",
        "    return top_k_item_ids[0:K],artist_recommendations.iloc[0:K,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_recommendations_new(model, user_id,usr_emb, item_emb, candidate_item_ids, K):\n",
        "    \"\"\"\n",
        "    Returns the top K recommended item IDs for a given user.\n",
        "\n",
        "    Args:\n",
        "        model: Trained NeuralCollaborativeFiltering model.\n",
        "        user_id: The user ID for which recommendations are needed.\n",
        "        candidate_item_ids: List (or tensor) of candidate item IDs.\n",
        "        K: The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top K recommended item IDs.\n",
        "    \"\"\"\n",
        "    # Create a tensor for the user repeated for each candidate item.\n",
        "    user_tensor = torch.tensor([user_id] * len(candidate_item_ids), dtype=torch.long)\n",
        "    item_tensor = torch.tensor(candidate_item_ids, dtype=torch.long)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        usr_emb = usr_emb[user_tensor]\n",
        "        item_emb = item_emb[item_tensor]\n",
        "        #------\n",
        "        # print(\"user_emb dtype:\", user_emb.dtype)\n",
        "        # print(\"item_emb dtype:\", item_emb.dtype)\n",
        "\n",
        "        x_hat = torch.cat([usr_emb, item_emb], dim=-1)\n",
        "        # x_hat = x_hat.float()\n",
        "        # print(type(x_hat))\n",
        "        y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "        # print(type(y_hat))\n",
        "        scores = y_hat.squeeze().detach().clone()\n",
        "\n",
        "    # Sort candidate items based on their scores (descending order).\n",
        "    # torch.argsort returns indices that would sort the tensor.\n",
        "    sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "        # Select the top K item IDs.\n",
        "    top_k_indices = sorted_indices[:K+40]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    # bottom_k_indices = sorted_indices[-(K+40):-1]\n",
        "    # top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    recommendations = [(user_artist_matrix.columns[top_k_item_ids[i]], round(float(scores[top_k_item_ids[i]].data),6)) for i in range(K+40)]\n",
        "\n",
        "    artist_recommendations = [artistt[0] for artistt in recommendations] #pd.DataFrame(df_user_emb@(df_item_emb.T)).iloc[user_id,:].sort_values(ascending = False)\n",
        "    artist_recommendations = merged_df__all.loc[artist_recommendations,:]\n",
        "\n",
        "    artist_recommendations = artist_recommendations.loc[list(set(artist_recommendations.index)- set(user_artist_matrix.columns[np.where(user_artist_matrix.iloc[user_id,:]==1)[0]])),:]\n",
        "    top_k_item_ids = user_artist_matrix.columns.get_indexer(artist_recommendations.index)\n",
        "    return scores, top_k_item_ids,recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_recommendations_new_all(model, user_id,usr_emb, item_emb, candidate_item_ids, K):\n",
        "    \"\"\"\n",
        "    Returns the top K recommended item IDs for a given user.\n",
        "\n",
        "    Args:\n",
        "        model: Trained NeuralCollaborativeFiltering model.\n",
        "        user_id: The user ID for which recommendations are needed.\n",
        "        candidate_item_ids: List (or tensor) of candidate item IDs.\n",
        "        K: The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top K recommended item IDs.\n",
        "    \"\"\"\n",
        "    # Create a tensor for the user repeated for each candidate item.\n",
        "    user_tensor = torch.tensor([user_id] * len(candidate_item_ids), dtype=torch.long)\n",
        "    item_tensor = torch.tensor(candidate_item_ids, dtype=torch.long)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        usr_emb = usr_emb[user_tensor]\n",
        "        item_emb = item_emb[item_tensor]\n",
        "        #------\n",
        "        # print(\"user_emb dtype:\", user_emb.dtype)\n",
        "        # print(\"item_emb dtype:\", item_emb.dtype)\n",
        "\n",
        "        x_hat = torch.cat([usr_emb, item_emb], dim=-1)\n",
        "        # x_hat = x_hat.float()\n",
        "        # print(type(x_hat))\n",
        "        y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "        # print(type(y_hat))\n",
        "        scores = y_hat.squeeze().detach().clone()\n",
        "\n",
        "    # Sort candidate items based on their scores (descending order).\n",
        "    # torch.argsort returns indices that would sort the tensor.\n",
        "    sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "        # Select the top K item IDs.\n",
        "    top_k_indices = sorted_indices[:K]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    # bottom_k_indices = sorted_indices[-(K+40):-1]\n",
        "    # top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    recommendations = [(user_artist_matrix.columns[top_k_item_ids[i]], round(float(scores[top_k_item_ids[i]].data),6)) for i in range(K)]\n",
        "\n",
        "    artist_recommendations = [artistt[0] for artistt in recommendations] #pd.DataFrame(df_user_emb@(df_item_emb.T)).iloc[user_id,:].sort_values(ascending = False)\n",
        "    artist_recommendations = merged_df__all.loc[artist_recommendations,:]\n",
        "\n",
        "    # artist_recommendations = artist_recommendations.loc[list(set(artist_recommendations.index)- set(user_artist_matrix.columns[np.where(user_artist_matrix.iloc[user_id,:]==1)[0]])),:]\n",
        "    top_k_item_ids = user_artist_matrix.columns.get_indexer(artist_recommendations.index)\n",
        "    return scores, top_k_item_ids,recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvG_W597p6Ua"
      },
      "outputs": [],
      "source": [
        "def get_top_k_recommendations_flex(model, user_id, candidate_item_ids, K):\n",
        "    \"\"\"\n",
        "    Returns the top K recommended item IDs for a given user.\n",
        "\n",
        "    Args:\n",
        "        model: Trained NeuralCollaborativeFiltering model.\n",
        "        user_id: The user ID for which recommendations are needed.\n",
        "        candidate_item_ids: List (or tensor) of candidate item IDs.\n",
        "        K: The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top K recommended item IDs.\n",
        "    \"\"\"\n",
        "    # Create a tensor for the user repeated for each candidate item.\n",
        "    user_tensor = torch.tensor([user_id] * len(candidate_item_ids), dtype=torch.long)\n",
        "    item_tensor = torch.tensor(candidate_item_ids, dtype=torch.long)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #-----\n",
        "        user_emb = model.user_embedding(user_tensor).detach().clone()\n",
        "        item_emb = model.item_embedding(item_tensor).detach().clone()\n",
        "        user_rec, user_encoded = sae_model(user_emb)\n",
        "        item_rec, item_encoded = sae_model(item_emb)\n",
        "        #------\n",
        "        # print(\"user_emb dtype:\", user_emb.dtype)\n",
        "        # print(\"item_emb dtype:\", item_emb.dtype)\n",
        "\n",
        "        x_hat = torch.cat([user_rec, item_rec], dim=-1)\n",
        "        # x_hat = x_hat.float()\n",
        "        # print(type(x_hat))\n",
        "        y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "        # print(type(y_hat))\n",
        "        scores = y_hat.squeeze().detach().clone()\n",
        "\n",
        "    # Sort candidate items based on their scores (descending order).\n",
        "    # torch.argsort returns indices that would sort the tensor.\n",
        "    sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "    # Select the top K item IDs.\n",
        "    top_k_indices = sorted_indices[:K]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    bottom_k_indices = sorted_indices[-K:-1]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    recommendations = [(user_artist_matrix.columns[top_k_item_ids[i]], round(float(scores[top_k_item_ids[i]].data),6)) for i in range(K)]\n",
        "    return scores, top_k_item_ids,recommendations\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "def get_top_k_recommendations_flex_(model, user_id, candidate_item_ids, K,item_rec_replace, item_id_replace):\n",
        "    \"\"\"\n",
        "    Returns the top K recommended item IDs for a given user.\n",
        "    insert item_rec_replace and item_id_replace.\n",
        "    Args:\n",
        "        model: Trained NeuralCollaborativeFiltering model.\n",
        "        user_id: The user ID for which recommendations are needed.\n",
        "        candidate_item_ids: List (or tensor) of candidate item IDs.\n",
        "        K: The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top K recommended item IDs.\n",
        "    \"\"\"\n",
        "    # Create a tensor for the user repeated for each candidate item.\n",
        "    user_tensor = torch.tensor([user_id] * len(candidate_item_ids), dtype=torch.long)\n",
        "    item_tensor = torch.tensor(candidate_item_ids, dtype=torch.long)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation.\n",
        "    model.eval()\n",
        "    sae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get predictions for all candidate items.\n",
        "        # scores  = model(user_tensor, item_tensor)\n",
        "        # print(f'logits: {scores}')\n",
        "        # scores = torch.sigmoid(logits)\n",
        "        #-----\n",
        "        user_emb = model.user_embedding(user_tensor).detach().clone()\n",
        "        item_emb = model.item_embedding(item_tensor).detach().clone()\n",
        "        user_rec, user_encoded = sae_model(user_emb)\n",
        "        item_rec, item_encoded = sae_model(item_emb)\n",
        "        #------\n",
        "        # print(\"user_emb dtype:\", user_emb.dtype)\n",
        "        # print(\"item_emb dtype:\", item_emb.dtype)\n",
        "        item_rec_replace = item_rec_replace.unsqueeze(-1)\n",
        "        item_rec[item_id_replace,:] = item_rec_replace.t()\n",
        "\n",
        "        x_hat = torch.cat([user_rec, item_rec], dim=-1)\n",
        "        # x_hat = x_hat.float()\n",
        "        # print(type(x_hat))\n",
        "        y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "        # print(type(y_hat))\n",
        "        scores = y_hat.squeeze().detach().clone()\n",
        "\n",
        "    # Sort candidate items based on their scores (descending order).\n",
        "    # torch.argsort returns indices that would sort the tensor.\n",
        "    sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "    # Select the top K item IDs.\n",
        "    top_k_indices = sorted_indices[:K]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    bottom_k_indices = sorted_indices[-K:-1]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    recommendations = [(user_artist_matrix.columns[top_k_item_ids[i]], round(float(scores[top_k_item_ids[i]].data),6)) for i in range(K)]\n",
        "    return scores, top_k_item_ids,recommendations\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "def get_top_k_recommendations_flex_user(model, user_id, candidate_item_ids, K,user_latent_replace):\n",
        "    \"\"\"\n",
        "    Returns the top K recommended item IDs for a given user.\n",
        "    insert user_latent_replace.\n",
        "    Args:\n",
        "        model: Trained NeuralCollaborativeFiltering model.\n",
        "        user_id: The user ID for which recommendations are needed.\n",
        "        candidate_item_ids: List (or tensor) of candidate item IDs.\n",
        "        K: The number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of the top K recommended item IDs.\n",
        "    \"\"\"\n",
        "    # Create a tensor for the user repeated for each candidate item.\n",
        "    user_tensor = torch.tensor([user_id] * len(candidate_item_ids), dtype=torch.long)\n",
        "    item_tensor = torch.tensor(candidate_item_ids, dtype=torch.long)\n",
        "\n",
        "    # Set the model to evaluation mode and disable gradient computation.\n",
        "    model.eval()\n",
        "    sae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get predictions for all candidate items.\n",
        "        # scores  = model(user_tensor, item_tensor)\n",
        "        # print(f'logits: {scores}')\n",
        "        # scores = torch.sigmoid(logits)\n",
        "        #-----\n",
        "        user_emb_modificate = user_latent_replace.repeat(6039, 1)\n",
        "        item_emb = model.item_embedding(item_tensor).detach().clone()\n",
        "        user_rec, user_encoded = sae_model(user_emb_modificate)\n",
        "        item_rec, item_encoded = sae_model(item_emb)\n",
        "        #-----\n",
        "\n",
        "        x_hat = torch.cat([user_rec, item_rec], dim=-1)\n",
        "        # x_hat = x_hat.float()\n",
        "        # print(type(x_hat))\n",
        "        y_hat = model.fc_layers(x_hat)  # (batch, 1)\n",
        "        # print(type(y_hat))\n",
        "        scores = y_hat.squeeze(-1 ).detach().clone()\n",
        "\n",
        "    # Sort candidate items based on their scores (descending order).\n",
        "    # torch.argsort returns indices that would sort the tensor.\n",
        "    sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "    # Select the top K item IDs.\n",
        "    top_k_indices = sorted_indices[:K]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    bottom_k_indices = sorted_indices[-K:-1]\n",
        "    top_k_item_ids = [candidate_item_ids[i] for i in top_k_indices]\n",
        "    recommendations = [(user_artist_matrix.columns[top_k_item_ids[i]], round(float(scores[top_k_item_ids[i]].data),6)) for i in range(K)]\n",
        "    return scores, top_k_item_ids,recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRmvo_vCr5rM"
      },
      "source": [
        "## Evaluation metrics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfvUduMpt_aH"
      },
      "source": [
        "NDCG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UQ680yVuAe_"
      },
      "outputs": [],
      "source": [
        "def ndcg_calc(k, model_type = 'MF', pos_idx_ex_hidden, model=model):\n",
        "\n",
        "  ndcg_sum = 0\n",
        "  ndcg_sum_all=[]\n",
        "  total = 0\n",
        "\n",
        "  for user_id in range(len(pos_idx_ex_hidden)):\n",
        "      hidden_items = pos_idx_ex_hidden[user_id]  # The relevant (hidden) items for the user\n",
        "      if model_type == 'MF':\n",
        "        recommendations = mf_recommender.recommend(user_id, k)\n",
        "      else:\n",
        "        recommendations = get_top_k_recommendations(model, user_id, list(range(2277)), k)[2]\n",
        "\n",
        "      # Compute DCG\n",
        "      dcg = 0\n",
        "      for rank, (item_id, score) in enumerate(recommendations):\n",
        "          if item_id in hidden_items:\n",
        "            # rank is the place of the recoommendation\n",
        "              dcg += score / np.log2(rank + 1 + 1)  # rank + 2 because rank starts from 0\n",
        "\n",
        "      # Compute IDCG (Ideal DCG)\n",
        "      idcg = sum(1 / np.log2(i + 2) for i in range(min(len(hidden_items), k)))\n",
        "\n",
        "      ndcg = dcg / idcg if idcg > 0 else 0\n",
        "      ndcg_sum += ndcg\n",
        "      ndcg_sum_all.append(ndcg)\n",
        "      total += 1\n",
        "\n",
        "  return avg_NDCG_at_k, ndcg_sum_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKGel2YvKFAc"
      },
      "source": [
        "MRR at k=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjc5iMRnwoFC"
      },
      "outputs": [],
      "source": [
        "def mmr_calc(k, model_type = 'MF', pos_idx_ex_hidden, model=model):\n",
        "\n",
        "  rr_sum = 0\n",
        "  total = 0\n",
        "\n",
        "  for user_id in range(len(pos_idx_ex_hidden)):\n",
        "    hidden_items = pos_idx_ex_hidden[user_id]\n",
        "    if model_type == 'MF':\n",
        "        recommendations = mf_recommender.recommend(user_id, k)\n",
        "    else:\n",
        "        recommendations = get_top_k_recommendations(model, user_id, list(range(2277)), k)[2]\n",
        "    used_flag = 0\n",
        "    for item_id in hidden_items:\n",
        "        for rank, (rec_item_id, _) in enumerate(recommendations):\n",
        "            if rec_item_id == item_id and used_flag==0:\n",
        "                used_flag = 1\n",
        "                rr_sum += 1 / (rank + 1)  # add 1 since the counting starts\n",
        "                                          # here from 0\n",
        "    total += 1\n",
        "  mrr_at_k = rr_sum / len(pos_idx_ex_hidden)\n",
        "\n",
        "  return mrr_at_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjKk1B4DKFAc"
      },
      "source": [
        "Hit rate at k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIsA1Oi9xRyC"
      },
      "outputs": [],
      "source": [
        "def hit_rate_calc(k, model_type = 'MF', pos_idx_ex_hidden, model=model):\n",
        "\n",
        "  hit_rate_at_k = 0\n",
        "  num_user_w_rel_item = 0\n",
        "\n",
        "  for user_id in range(len(pos_idx_ex_hidden)):\n",
        "    hidden_items = pos_idx_ex_hidden[user_id]\n",
        "    if model_type == 'MF':\n",
        "        recommendations = mf_recommender.recommend(user_id, k)\n",
        "    else:\n",
        "        recommendations = get_top_k_recommendations(model, user_id, list(range(2277)), k)[2]\n",
        "    used_flag = 0\n",
        "    for item_id in hidden_items:\n",
        "        for rank, (rec_item_id, _) in enumerate(recommendations):\n",
        "            if rec_item_id == item_id and used_flag==0:\n",
        "                used_flag = 1\n",
        "                num_user_w_rel_item += 1\n",
        "  hit_rate_at_k = num_user_w_rel_item/len(pos_idx_ex_hidden)\n",
        "\n",
        "  return hit_rate_at_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P_EFobXKFAc"
      },
      "source": [
        "Mean percentile rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE0pN5wyyGTC"
      },
      "outputs": [],
      "source": [
        "def mpr_calc(model_type = 'MF', pos_idx_ex_hidden, model=model):\n",
        "\n",
        "  mean_percentile_rank = 0\n",
        "  percentile_rank = 0\n",
        "\n",
        "  for user_id in range(len(pos_idx_ex_hidden)):\n",
        "    hidden_items = pos_idx_ex_hidden[user_id]\n",
        "    if model_type == 'MF':\n",
        "        recommendations = mf_recommender.recommend(user_id, 3706)\n",
        "    else:\n",
        "        recommendations = get_top_k_recommendations(model, user_id, list(range(2277)), 3706)[2]\n",
        "\n",
        "    rr = 0\n",
        "    for item_id in hidden_items:\n",
        "        for rank, (rec_item_id, _) in enumerate(recommendations):\n",
        "            if rec_item_id == item_id:\n",
        "                rr += (rank/3706)*100\n",
        "    percentile_rank += rr/len(hidden_items)\n",
        "  mean_percentile_rank = percentile_rank/len(pos_idx_ex_hidden)\n",
        "\n",
        "  return mean_percentile_rank"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
