{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## This notebook contains models architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-jU-KhY6LHq"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "export_dir = os.getcwd()\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import plot\n",
        "import random\n",
        "import math\n",
        "import heapq\n",
        "from scipy.special import expit  # Sigmoid function\n",
        "import itertools\n",
        "from IPython.display import Latex, display\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "# Ignore FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "torch.set_printoptions(sci_mode=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rvEWZkEhWte"
      },
      "outputs": [],
      "source": [
        "pip install ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1CmF7q1h6iF"
      },
      "outputs": [],
      "source": [
        "from ipynb.fs.defs.utils import *\n",
        "from ipynb.fs.defs.data_processing import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6M-fL58mm_Z"
      },
      "source": [
        "## SAE NCF Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY1192JfmoXt"
      },
      "outputs": [],
      "source": [
        "class TopKActivation(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        \"\"\"\n",
        "        Keeps only the top k values (per sample) and zeros out the rest.\n",
        "        \"\"\"\n",
        "        super(TopKActivation, self).__init__()\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, hidden_dim)\n",
        "        # Get top-k values along the feature dimension.\n",
        "        topk_values, _ = torch.topk(x, self.k, dim=1)\n",
        "        # Threshold is the k-th largest value for each sample.\n",
        "        threshold = topk_values[:, -1].unsqueeze(1).expand_as(x)\n",
        "        mask = (x >= threshold).float()\n",
        "        return x * mask\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Custom Tied Transpose Module for Decoder\n",
        "class TiedTranspose(nn.Module):\n",
        "    def __init__(self, tied_layer):\n",
        "        \"\"\"\n",
        "        Ties this module's weight to the transpose of the given linear layer's weight.\n",
        "        \"\"\"\n",
        "        super(TiedTranspose, self).__init__()\n",
        "        self.tied_layer = tied_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use the transpose of the tied layer's weight.\n",
        "        return nn.functional.linear(x, self.tied_layer.weight.t())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJVOyXbXmzjD"
      },
      "outputs": [],
      "source": [
        "class SparseAutoencoderNCF(nn.Module):\n",
        "    def __init__(self, input_dim=100, hidden_dim=70, topk=5, tie_weights=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): Dimensionality of each input embedding (20).\n",
        "            hidden_dim (int): Dimensionality of the latent space.\n",
        "            topk (int): Number of activations to keep per sample- if using topK activation.\n",
        "            tie_weights (bool): If True, tie the decoder's weight to the encoder's weight.\n",
        "        \"\"\"\n",
        "        super(SparseAutoencoderNCF, self).__init__()\n",
        "        # Encoder: a linear layer followed by a activation.\n",
        "        self.encoder_linear = nn.Linear(input_dim, hidden_dim)\n",
        "        # self.topk_activation = TopKActivation(k=topk)\n",
        "        self.topk_activation = nn.ReLU()\n",
        "\n",
        "\n",
        "        # Decoder: if tie_weights is True, use TiedTranspose.\n",
        "        self.tie_weights = tie_weights\n",
        "        if tie_weights:\n",
        "            self.decoder = TiedTranspose(self.encoder_linear)\n",
        "        else:\n",
        "            self.decoder = nn.Linear(hidden_dim, input_dim, bias=False)\n",
        "        self.loss = []\n",
        "        self.weights_loss = []\n",
        "        self.test_subset_users_ind = test_subset_users\n",
        "        self.test_subset_items_ind = test_subset_items\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        latent_pre_act = self.encoder_linear(x)           # (batch, hidden_dim)\n",
        "        encoded = self.topk_activation(latent_pre_act)      # (batch, hidden_dim)\n",
        "        decoded = self.decoder(encoded)                     # (batch, input_dim)\n",
        "        return decoded, encoded\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5EER_PmHV5K"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Q5XYZMnNCH"
      },
      "source": [
        "## MF SAE Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw4eEubZnNCI"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "\n",
        "      def __init__(\n",
        "          self, n_latents: int, n_inputs: int, activation: Callable = nn.ReLU(), tied: bool = True,\n",
        "          normalize: bool = False\n",
        "      ) -> None:\n",
        "          super().__init__()\n",
        "\n",
        "          self.pre_bias = nn.Parameter(torch.zeros(n_inputs))\n",
        "          self.encoder: nn.Module = nn.Linear(n_inputs, n_latents, bias=False)\n",
        "          self.latent_bias = nn.Parameter(torch.zeros(n_latents))\n",
        "          self.activation = activation\n",
        "          if tied:\n",
        "              self.decoder: nn.Linear | TiedTranspose = TiedTranspose(self.encoder)\n",
        "          else:\n",
        "              self.decoder = nn.Linear(n_latents, n_inputs, bias=False)\n",
        "          self.normalize = normalize\n",
        "          self.loss = []\n",
        "          self.test_subset_users_ind = test_subset_users ####\n",
        "          self.test_subset_items_ind = test_subset_items ####\n",
        "          self.weights_loss = [] ####\n",
        "          self.test = test_flag\n",
        "          self.activation_rate = {}\n",
        "\n",
        "\n",
        "      def encode_pre_act(self, x: torch.Tensor, latent_slice: slice = slice(None)) -> torch.Tensor:\n",
        "          if type(x) == np.ndarray:\n",
        "            x= torch.from_numpy(x)\n",
        "          x = x - self.pre_bias\n",
        "          latents_pre_act = F.linear(\n",
        "              x, self.encoder.weight[latent_slice], self.latent_bias[latent_slice]\n",
        "          )\n",
        "          return latents_pre_act\n",
        "\n",
        "      def preprocess(self, x: torch.Tensor) -> tuple[torch.Tensor, dict[str, Any]]:\n",
        "          if not self.normalize:\n",
        "              return x, dict()\n",
        "          x, mu, std = LN(x)\n",
        "          return x, dict(mu=mu, std=std)\n",
        "\n",
        "      def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, dict[str, Any]]:\n",
        "          x, info = self.preprocess(x)\n",
        "          return self.activation(self.encode_pre_act(x)), info\n",
        "\n",
        "      def decode(self, latents: torch.Tensor, info: dict[str, Any] | None = None) -> torch.Tensor:\n",
        "          ret = self.decoder(latents) + self.pre_bias\n",
        "          if self.normalize:\n",
        "              assert info is not None\n",
        "              ret = ret * info[\"std\"] + info[\"mu\"]\n",
        "          return ret\n",
        "\n",
        "      def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "          x, info = self.preprocess(x)\n",
        "          latents_pre_act = self.encode_pre_act(x)\n",
        "          latents = self.activation(latents_pre_act)\n",
        "          recons = self.decode(latents, info)\n",
        "          return latents_pre_act, latents, recons\n",
        "\n",
        "      @classmethod\n",
        "      def from_state_dict(\n",
        "          cls, state_dict: dict[str, torch.Tensor], strict: bool = True\n",
        "      ) -> \"Autoencoder\":\n",
        "          n_latents, d_model = state_dict[\"encoder.weight\"].shape\n",
        "\n",
        "          # Retrieve activation\n",
        "          activation_class_name = state_dict.pop(\"activation\", \"ReLU\")\n",
        "          activation_class = ACTIVATIONS_CLASSES.get(activation_class_name, nn.ReLU)\n",
        "          normalize = activation_class_name == \"TopK\"\n",
        "          activation_state_dict = state_dict.pop(\"activation_state_dict\", {})\n",
        "          if hasattr(activation_class, \"from_state_dict\"):\n",
        "              activation = activation_class.from_state_dict(\n",
        "                  activation_state_dict, strict=strict\n",
        "              )\n",
        "          else:\n",
        "              activation = activation_class()\n",
        "              if hasattr(activation, \"load_state_dict\"):\n",
        "                  activation.load_state_dict(activation_state_dict, strict=strict)\n",
        "\n",
        "          autoencoder = cls(n_latents, d_model, activation=activation, normalize=normalize)\n",
        "          autoencoder.load_state_dict(state_dict, strict=strict)\n",
        "          return autoencoder\n",
        "\n",
        "      def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n",
        "          sd = super().state_dict(destination, prefix, keep_vars)\n",
        "          sd[prefix + \"activation\"] = self.activation.__class__.__name__\n",
        "          if hasattr(self.activation, \"state_dict\"):\n",
        "              sd[prefix + \"activation_state_dict\"] = self.activation.state_dict()\n",
        "          return sd\n",
        "\n",
        "\n",
        "class TiedTranspose(nn.Module):\n",
        "    def __init__(self, linear: nn.Linear):\n",
        "        super().__init__()\n",
        "        self.linear = linear\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert self.linear.bias is None\n",
        "        return F.linear(x, self.linear.weight.t(), None)\n",
        "\n",
        "    @property\n",
        "    def weight(self) -> torch.Tensor:\n",
        "        return self.linear.weight.t()\n",
        "\n",
        "    @property\n",
        "    def bias(self) -> torch.Tensor:\n",
        "        return self.linear.bias\n",
        "\n",
        "\n",
        "class TopK(nn.Module):\n",
        "    def __init__(self, k: int, postact_fn: Callable = nn.ReLU()) -> None:\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.postact_fn = postact_fn\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        topk = torch.topk(x, k=self.k, dim=-1)\n",
        "        values = self.postact_fn(topk.values)\n",
        "        result = torch.zeros_like(x)\n",
        "        result.scatter_(-1, topk.indices, values)\n",
        "        return result\n",
        "\n",
        "    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n",
        "        state_dict = super().state_dict(destination, prefix, keep_vars)\n",
        "        state_dict.update({prefix + \"k\": self.k, prefix + \"postact_fn\": self.postact_fn.__class__.__name__})\n",
        "        return state_dict\n",
        "\n",
        "    @classmethod\n",
        "    def from_state_dict(cls, state_dict: dict[str, torch.Tensor], strict: bool = True) -> \"TopK\":\n",
        "        k = state_dict[\"k\"]\n",
        "        postact_fn = ACTIVATIONS_CLASSES[state_dict[\"postact_fn\"]]()\n",
        "        return cls(k=k, postact_fn=postact_fn)\n",
        "\n",
        "\n",
        "ACTIVATIONS_CLASSES = {\n",
        "    \"ReLU\": nn.ReLU,\n",
        "    \"Identity\": nn.Identity,\n",
        "    \"TopK\": TopK,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAE Matryoshka Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "class MatryoshkaAutoencoder(Autoencoder):\n",
        "    def __init__(self,\n",
        "                 latent_dim: int,\n",
        "                 input_dim: int,\n",
        "                 group_sizes: list[int],\n",
        "                 activation: nn.Module = nn.ReLU(),\n",
        "                 tied: bool = True,\n",
        "                 normalize: bool = True):\n",
        "        \"\"\"\n",
        "        Extends your base Autoencoder to implement Matryoshka-style nested codebooks.\n",
        "\n",
        "        Args:\n",
        "            latent_dim: total number of latent units (D)\n",
        "            input_dim:  dimension of input features (e.g. K from MF embeddings)\n",
        "            group_sizes: list of prefix lengths [m1, m2, ..., D] that sum to latent_dim\n",
        "            activation:  nonlinearity to apply after encoder pre-activation\n",
        "            tied:        whether decoder weights are tied to encoder\n",
        "            normalize:   whether to apply normalization hooks from base class\n",
        "        \"\"\"\n",
        "        super().__init__(latent_dim, input_dim,\n",
        "                         activation=activation,\n",
        "                         tied=tied, normalize=normalize)\n",
        "        assert group_sizes[-1] == latent_dim\n",
        "        self.group_sizes = group_sizes\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Preprocess and encode\n",
        "        x_norm, info = self.preprocess(x)\n",
        "        z_pre  = self.encode_pre_act(x_norm)\n",
        "        z = self.activation(z_pre)  # apply activation (ReLU)\n",
        "\n",
        "        # Multi-level reconstruction: for each prefix length m,\n",
        "        # mask off latents beyond m and decode\n",
        "        recons = []\n",
        "        for m in self.group_sizes:\n",
        "            z_masked = z.clone()\n",
        "            z_masked[:, m:] = 0  # zero out units beyond prefix m\n",
        "            x_hat = self.decode(z_masked, info)\n",
        "            recons.append(x_hat)\n",
        "        return z_pre, z, recons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40FXnwFoZRXN"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gyLso2Tm9PL"
      },
      "source": [
        "## MF recommender Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xlySSz9wZV0"
      },
      "outputs": [],
      "source": [
        "class MatrixFactorization:\n",
        "    def __init__(self, R, R_df, pos_idx_ex_use, neg_idx_ex_use, neg_ex_hidden, neg_ex, pos_ex_num, K, alpha, beta, iterations):\n",
        "\n",
        "        self.R = R\n",
        "        self.ratings = R_df\n",
        "        self.num_users = R.shape[0]\n",
        "        self.num_items = R.shape[1]\n",
        "        self.K = K  # latent dimensions (e.g., 100)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.iterations = iterations\n",
        "\n",
        "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
        "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
        "\n",
        "        # Initialize biases\n",
        "        self.b_u = np.zeros(self.num_users)\n",
        "        self.b_i = np.zeros(self.num_items)\n",
        "\n",
        "        self.pos_idx_ex_use = pos_idx_ex_use_\n",
        "        self.neg_idx_ex_use = neg_idx_ex_use_\n",
        "        self.neg_ex = neg_ex_\n",
        "        self.neg_ex_hidden = neg_ex_hidden_\n",
        "        self.neg_ex_use = {(row): list(filter(lambda x: x not in self.neg_ex_hidden[row],\n",
        "                                self.neg_ex[row])) for row in range(self.num_users)}\n",
        "        self.pos_ex_num = pos_ex_num_\n",
        "\n",
        "        # test set to track validation error through training:\n",
        "        self.rmse_train = {}\n",
        "        self.rmse_test = {}\n",
        "\n",
        "        self.columns = list(self.ratings.columns)\n",
        "        # To store batch logs across epochs if needed.\n",
        "        self.batch_history = []\n",
        "\n",
        "\n",
        "    def sgd_batch_tensor(self, u_batch, i_batch, r_batch):\n",
        "      pred = torch.sigmoid(self.b_u[u_batch] + self.b_i[i_batch] +\n",
        "                          torch.sum(self.P[u_batch] * self.Q[i_batch], dim=1))\n",
        "      error = r_batch - pred\n",
        "\n",
        "      grad_b_u = self.alpha * (error - self.beta * self.b_u[u_batch])\n",
        "      grad_b_i = self.alpha * (error - self.beta * self.b_i[i_batch])\n",
        "      grad_P = self.alpha * (error.unsqueeze(1) * self.Q[i_batch] - self.beta * self.P[u_batch])\n",
        "      grad_Q = self.alpha * (error.unsqueeze(1) * self.P[u_batch] - self.beta * self.Q[i_batch])\n",
        "\n",
        "\n",
        "\n",
        "      # Gradient accumulation\n",
        "      self.b_u.index_add_(0, u_batch, grad_b_u)\n",
        "      self.b_i.index_add_(0, i_batch, grad_b_i)\n",
        "      self.P.index_add_(0, u_batch, grad_P)\n",
        "      self.Q.index_add_(0, i_batch, grad_Q)\n",
        "\n",
        "\n",
        "    def rmse_tensor(self, user_ids, item_ids, labels):\n",
        "        \n",
        "        # prediction calculation\n",
        "\n",
        "        user_ids = user_ids.long()\n",
        "        item_ids = item_ids.long()\n",
        "\n",
        "\n",
        "        pred = torch.sigmoid(\n",
        "            self.b_u[user_ids] +\n",
        "            self.b_i[item_ids] +\n",
        "            torch.sum(self.P[user_ids] * self.Q[item_ids], dim=1)\n",
        "        )\n",
        "\n",
        "        # validation examples\n",
        "        mask = labels == 1\n",
        "        if mask.sum() == 0:\n",
        "            return float('nan')  # no validation examples\n",
        "\n",
        "        error = labels[mask] - pred[mask]\n",
        "        mse = (error ** 2).mean()\n",
        "        return torch.sqrt(mse).item()\n",
        "\n",
        "\n",
        "    def get_rating(self, i, j):\n",
        "        return torch.sigmoid(\n",
        "            self.b_u[i] + self.b_i[j] + torch.sum(self.P[i] * self.Q[j])\n",
        "        ).item()\n",
        "\n",
        "\n",
        "    def full_matrix(self):\n",
        "        return torch.sigmoid(\n",
        "            self.b_u[:, None] + self.b_i[None, :] + self.P @ self.Q.T\n",
        "        )\n",
        "\n",
        "    def recommend(self, user_id, top_n):\n",
        "        df_user_ratings = pd.DataFrame(self.full_matrix(), index=self.ratings.index, columns=self.ratings.columns)\n",
        "        df_user_ratings_one = df_user_ratings.loc[user_id]\n",
        "        recommendations = [(i, df_user_ratings_one.loc[i]) for i in df_user_ratings.columns]\n",
        "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "        return recommendations[:top_n]\n",
        "\n",
        "    # def recommend_pop_regularized(self, user_id, top_n, filter_listened=False, pop_penalty=pop_penalty):\n",
        "    def recommend_pop_regularized(self, user_id, top_n, filter_listened=False):\n",
        "\n",
        "      if not hasattr(self, 'predicted_matrix'):\n",
        "          self.predicted_matrix = self.full_matrix()\n",
        "\n",
        "      df_user_ratings = pd.DataFrame(self.predicted_matrix, index=self.ratings.index, columns=self.ratings.columns)\n",
        "      df_user_ratings_one = df_user_ratings.loc[user_id]\n",
        "\n",
        "      if filter_listened:\n",
        "          already_listened = set(self.R.columns[self.R.loc[user_id] > 0])\n",
        "          candidate_items = [i for i in df_user_ratings.columns if i not in already_listened]\n",
        "      else:\n",
        "          candidate_items = df_user_ratings.columns\n",
        "      recommendations = [(i, df_user_ratings_one.loc[i]) for i in candidate_items]\n",
        "      recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "      return recommendations[:top_n]\n",
        "\n",
        "\n",
        "    def recommend_norm(self, user_id, top_n):\n",
        "        return normalize_val(self.recommend(user_id, top_n))\n",
        "\n",
        "    \n",
        "    def recommend_unknown(self, user_id, top_n, filter_listened=True):\n",
        "\n",
        "      if not hasattr(self, 'predicted_matrix'):\n",
        "          self.predicted_matrix = self.full_matrix()\n",
        "\n",
        "      df_user_ratings = pd.DataFrame(self.predicted_matrix, index=self.R.index, columns=self.R.columns)\n",
        "      df_user_ratings_one = df_user_ratings.loc[user_id]\n",
        "\n",
        "      if filter_listened:\n",
        "          already_listened = set(self.R.columns[self.R.loc[user_id] > 0])\n",
        "          candidate_items = [i for i in df_user_ratings.columns if i not in already_listened]\n",
        "      else:\n",
        "          candidate_items = df_user_ratings.columns\n",
        "      recommendations = [(i, df_user_ratings_one.loc[i]) for i in candidate_items]\n",
        "      recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "      return recommendations[:top_n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gApW9fkZP1S"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjgkFFLBm-lg"
      },
      "source": [
        "## NCF recommender Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywa8PHXznMSU"
      },
      "outputs": [],
      "source": [
        "class NeuralCollaborativeFiltering(nn.Module):\n",
        "    def __init__(self, num_users=user_artist_matrix.shape[0], num_items=user_artist_matrix.shape[1], embedding_dim=100, hidden_layers=[64, 32, 16]):\n",
        "        \"\"\"\n",
        "        num_users: total number of users\n",
        "        num_items: total number of items\n",
        "        embedding_dim: dimensionality of the latent representation\n",
        "        hidden_layers: list containing the sizes of the hidden layers of the MLP\n",
        "        \"\"\"\n",
        "        super(NeuralCollaborativeFiltering, self).__init__()\n",
        "\n",
        "\n",
        "        #------------------------------------------------------------\n",
        "        # item and user embeddings initializtion:\n",
        "\n",
        "        # pretrained_weights_item = dataset_items_init.clone().detach() \n",
        "        # # Create the embedding layer:\n",
        "        # item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        # # Copy the pretrained weights into the embedding layer:\n",
        "        # item_embedding.weight.data.copy_(pretrained_weights_item)\n",
        "\n",
        "\n",
        "        # pretrained_weights_user = dataset_users_init.clone().detach() \n",
        "        # # Create the embedding layer:\n",
        "        # user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        # # Copy the pretrained weights into the embedding layer:\n",
        "        # user_embedding.weight.data.copy_(pretrained_weights_user)\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        nn.init.normal_(self.user_embedding.weight, mean=0.0, std=0.1)\n",
        "        nn.init.normal_(self.item_embedding.weight, mean=0.0, std=0.1)\n",
        "\n",
        "        self.user_embedding = user_embedding\n",
        "        self.item_embedding = item_embedding\n",
        "        self.user_bias   = nn.Embedding(num_users, 1)\n",
        "        self.item_bias   = nn.Embedding(num_items, 1)\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "\n",
        "        #------------------------------------------------------------\n",
        "\n",
        "        self.pos_idx_ex_use = pos_idx_ex_use_\n",
        "        self.neg_idx_ex_use = neg_idx_ex_use_\n",
        "        self.neg_ex = neg_ex_\n",
        "        self.neg_ex_hidden= neg_ex_hidden_\n",
        "        self.pos_ex_num = pos_ex_num_\n",
        "        self.neg_ex_use = neg_ex_use_\n",
        "\n",
        "        # MLP:\n",
        "        layers = []\n",
        "        input_dim = embedding_dim * 2  # Concatenation of user and item embeddings\n",
        "        for hidden_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            input_dim = hidden_dim\n",
        "        # Final output layer to produce the rating or score\n",
        "        layers.append(nn.Linear(input_dim, 1))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.fc_layers = nn.Sequential(*layers)\n",
        "        \n",
        "        nn.init.zeros_(self.user_bias.weight)\n",
        "        nn.init.zeros_(self.item_bias.weight)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        # Look up embeddings\n",
        "        user_emb = self.user_embedding(user_indices)\n",
        "        item_emb = self.item_embedding(item_indices)\n",
        "        # Concatenate embeddings instead of taking inner product.\n",
        "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
        "        # Pass through the MLP to obtain prediction.\n",
        "        output = self.fc_layers(x).squeeze(-1)  + self.user_bias(user_indices).squeeze(-1) \\\n",
        "                        + self.item_bias(item_indices).squeeze(-1)\n",
        "        return output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
